<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[这三年]]></title>
    <url>%2F2022%2F10%2F22%2F%E8%BF%99%E4%B8%89%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[已经快有三年没有更新博客了，三年，你知道这三年我怎么过吗? 背景 不知不觉疫情已经持续三年了，相信新冠疫情对大家的生活都造成了或多或少的影响。对于我来说，博客的断更，直接原因也是因为20年初防疫隔离在出租屋，而博客构建所需要的本地目录没有放到git托管，渐渐的把我有博客这一回事都忘却了。当然中间也有域名续费的通知，但又由于种种原因没有进行写作，于是一断便是三年。 虽然这三年断更了，但是工作上做的事情可不少。是的，我还待在三年前的公司，最近闲下来，终于觉得我们做的东西慢慢的上了轨道，所以现在想详细写下来我以及我的团队，这三年来做的东西。 前言 在上一篇文章，总结了我19年的经历，也提及了最后我转到了内部应用组。达到一定规模的公司，其实有十分多的内部应用，因为每个部门都会把一些能信息化、流程化的东西做成系统。与其说我们是搞内部应用，不如说我们是搞关于职能部门的信息化系统更为准确。所以我们部门后来名称也改了，叫业务效能组。 我们负责的最主要的系统，就是OA和运营后台。由于运营后台系统牵扯的业务无比庞大，我们只是负责其中一部分模块，而OA则是完全由我们部门负责。我从一开始加入到我们部门，就是负责OA开发，虽然现在我已经是负责整个后端团队，但运营后台相关我相对介入较少，所以下面的内容主要围绕OA展开。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年终总结]]></title>
    <url>%2F2019%2F12%2F29%2F2019%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[心存希望，努力向阳 叨叨 以前的一些同事在群里面发了一下自己的年终总结，大家总结的开场白几乎一模一样，都是“时光荏苒，日月如梭”之类的，回顾了一下我去年的总结，似乎也是这样，所以今年就随意一点，不搞这些客套话了。 正文 今年已经是步入社会的第三个年头，年中的时候我换了一份工作，用专业的术语来说应该是跳槽到了新的公司。而换工作的原因，除了马云说的那些，也考虑到未来的职业发展。在小公司或者说初创公司，确实是能够锻炼人的各种能力。在小公司里面，为了解决问题，你需要当万金油，会接触到很多跨越了自己的技术领域的事情。但是同时很多这种跨领域的学习和使用都是临时性方案，仅仅会用到一两次，再也不会深入研究。其实很多文章里也讲过这些东西，我的总结毕竟不是分析大小公司的利弊，所以就也是叨叨罢了。 年初的时候，公司承接了广交会的一个项目，搞得如火如荼。整个项目持续一周多，每个同事都忙里忙外，大家都体验到举办大型展会的辛苦，但也可以说是一起经历了难忘的日子，运用浑身解数去解决问题。同样的经历对不同的人来说可能会有不同的意义，有的人得到了什么，而有的人又失去了什么，归根到底只是人的问题，经历本身可以是一样的，但是结果往往会不同。 三四月份的时候跟勇哥提了离职，并准备出去找工作，勇哥觉得比较惋惜，所以建议我可以先尝试找工作，有好的机会他也不会阻拦我，如果没什么好机会建议我留下，随后便开始了求职之旅了~其实也没有出去面试太多的公司，虽然是投递了几家比较感兴趣的公司，可惜 HR 似乎对我的履历不太感兴趣，有的简历直接就被筛掉了。最后倒还是得到了一些比较珍贵的面试机会，有的虽然在深圳，不过我知道机会得来不易得珍惜，所以跑半天也会过去。 具体的面试经历我就不说了，之前也写过一些了。总的来说，在一段时间内对某些知识的学习和加固是比较有用的，不过面试不全是考封闭性的题目，开放性的题目也会有，所以还是得扩展自己的知识面，保持接触业界的最前沿，还有就是面试的时候尽量让自己放松，紧张的时候真的跟没脑子没差别。有些面试官会故意施压，一般来说只是考验抗压能力，没必要畏惧。 过了大概一个月吧，拿到了些 offer ，最后选择了 BIGO ，实话实说就是钱比较多哈哈，毕竟有加班双薪，其他的各种福利也都不错，相比来说在广州确实算是一个比较好的公司。虽然工作时长不可同日而语了，但是我可能是有一些工作狂的属性加成，以前总觉得每天工作七八个小时好像一下子就过去了。大家看了当作玩笑话就可，有时间拿来陪伴家人会比工作来得更加让人幸福吧。 来到 BIGO 之后在 hello web 组待了四个月，基本做的是管理后台的一些工作，期间还跨部门做过两个系统。干了一段时间总算对内部的一些工具算熟门熟路了，刚好组长告诉我有个转 Java 的机会，问我考不考虑，对我来说语言也只是表达工具的一种，转不转的其实没什么差别，只是我觉得有机会接触新的业务和新的团队可能也不错，于是就同意转部门写起了 Java 了。 于是我加入到了内部应用组，开始参与 OA 的开发。 OA 的需求方有很多，包括且不限于 HR、财务、老板。我们每天都会有新的需求，也接受着来自各方的一些问题反馈，所以每天的工作任务是比较多的。虽然工作内容比较多，但是相对来说事情的紧急程度并不是那么高，所以相对来说压力也并不大。就这样在新部门待了又四个月，由于支付部门缺人，且任务紧急，我最近会有一波临时调度。 总结 由于今年工作时间占据大部分的时间，一些去年定下的计划没有很好的执行，入职后办了张健身卡，去了四五个月，最近也因为太多事情没有坚持下去，不过今年跟同学朋友的聚会倒是多了，团建吃饭的机会也很多，所以，又胖了😂。 总的来说，今年的工作基调好像从年初就确定了，变动得比较频繁，不过不管怎么样，生活还是得继续。 完。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树形结构数据的存储方式]]></title>
    <url>%2F2019%2F10%2F13%2F%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[之前我们说到了面向对象语言层面树形结构数据的抽象，今天我们那来看看树形结构的数据一般是如何存储在数据库里面的。 前言 最近对树形结构的数据有比较浓厚的研究学习兴趣，具体的原因嘛，嘿嘿你猜。 上一篇文章里面提到了之前我们的树形结构数据的存储方式，就是大家都很常用的pid关联父级id，想必这种存储方式是大多数人一贯的选择方案，毕竟这种存储方式很符合人类正常的思考模式，而且自顶而下的查询也是递归查询，跟我们日常习惯相符合。 然而人类的常规思维总有一些限制，当参数变得复杂就会变得十分低效，所以符合人类的思考模式的方式尽管能够让人易于理解，但却不一定是好的方式。 存储方式 所以回归到主题，对于树形结构的数据有哪些存储方式呢？这些存储方式中有没有最优的呢？如果没有那么各种方式分别适合于什么样的场景呢？又各有什么限制呢？ 下面内容是对 《SQL Antipatterns》 中提到的四种树形结构数据的存储方式的概括，我们带着以上的问题，尝试着去寻找答案。 Adjacency List 这种就是我们最常用的方式，这种存储方式在插入、修改和删除节点的时候会比较有优势，基本的表设计就是如下： id pid name 1 0 Level1 2 1 Level2 3 2 Level3 一般可以使用0作为所有节点的虚拟根节点，0节点可以不存在数据库中，查第一层的数据只要将pid=0的条件带上就可以了。得到结果集后再遍历所有并使用结果的id作为之后查询条件的pid条件进行查询，如此重复下去直到每个查询得到的结果集都为空，那么就完成了完整的一棵树的查询。每个查询完成你可以将数据放到数组里面，或者像上一篇所说自己定义树结构，最后得出一个带层级关系的多维数组或者树。 以上是想要获取整棵树的一般查询方式，用到就是递归，有多少个节点就需要查询多少次，除了由上而下的查询之外，由下而上的反向查询也是需要递归查询直到pid=0为止才能获取到所有相关的父级节点。所以大家可以看到这种存储和查询方式在节点和层级十分多的时候可能会出现效率的问题。 一开始说到这种存储方式在增删改的时候有优势，可能大家以往没有意识到这点，因为所谓的优势也是需要通过对比才能体现出来的，那么下面我们先来看看其他几种存储方式吧。 Path Enumeration 大家先看一下这种设计的表结构： id path name 1 1 Level1 2 1/2 Level2 3 1/2/3 Level3 这种方式就是将每个节点的完整路径存储下来，那么我们查询子所有的节点就需要如下SQL： select * from table where path like '1/%'; 没错使用的就是模糊查询，由于没有使用左通配符，索引还是能用的，这种方式下查询所有的父节点也非常方便，因为完整的路径已经查出来了，那么利用分隔符获得所有父节点的id再使用in查询就成了。当然反向like也行： select * from table where '1/2/3' like path || '%'; 当然这种方式局限性也非常的大，例如在你需要更改节点2的path为2，那么你还必须将节点3的path更新为2/3，关联的节点越多，需要更新的记录数就越多，删除节点同理；另外，由于path是存储每个节点的完整路径，所以一旦树的高度非常的高，那么path所存储的数据量就会很大，占用十分多的存储空间。 最后书里面还提到了一点就是脏数据的问题，因为这种存储方式在修改的插入的时候需要更改非常多的记录，另外还需要对path进行解析，比较容易出现格式异常、更改记录失败或者使用大事务，所以为了避免出现脏数据，就需要程序做到比较完善的校验规则。 Nested Sets 这种解决方案存储与属于其后代集的每个节点（而不是该节点的直接父级）有关的信息。 可以通过用两个数字对树中的每个节点进行编码来表示此信息，我们可以将其称为nsleft和nsright。 这种存储方式的表结构如下： id nsleft nsright name 1 1 14 node1 2 2 7 node2 3 3 4 node3 4 5 6 node4 5 8 13 node5 6 9 10 node6 7 11 12 node7 每个节点通过以下方式获得nsleft和nsright：nsleft小于所有节点的子代数，而nsright大于所有节点的子代数。关于如何分配这些值，一种比较简单方法是沿着树的深度优先遍历，当下降时，递增的分配nsleft，而当上升时，递增的分配nsright。 为每个节点分配了这些编号后，就可以使用它们来查找任何给定节点的祖先和后代。 例如，我们可以通过搜索编号在当前节点的nsleft和nsright之间的节点来检索id为2的节点及其后代。 SELECT t1.* FROM table t1 JOIN table t2 ON t2.nsleft BETWEEN t1.nsleft AND t1.nsright WHERE t1.id = 2; 可以通过搜索编号跨越当前节点编号的节点来检索id为6的节点及其祖先。 SELECT t1.* FROM table t1 JOIN table t2 ON t1.nsleft BETWEEN t2.nsleft AND t2.nsright WHERE t1.id = 6; 这种设计的主要优势在于，当删除一个非叶子节点时，其后代会自动被视为已删除节点的父节点的直接子节点。 尽管插图中显示的每个节点的左右数字具有形成连续序列的值，并且与相邻的兄弟节点和父节点相比，差异始终为1，但是对于该设计来说，保留层次结构并不是必需的。 因此，当删除节点导致数值不连续时，也并不会中断树结构。 这种存储模型最适合用于快速地对子树执行查询，而不利于对单个节点进行操作。 因为插入和移动节点很复杂，且需要重新编号nsleft和nsright。 如果频繁对树插入节点的话，则这种设计是十分不理想的。 Closure Table 这四种方法里面唯一一个需要两张表的存储方式，就是这个Closure Table了，老规矩我们先上表设计为敬。 id name 1 Level1 2 Level2 3 Level3 fid sid 1 1 1 2 1 3 2 2 2 3 3 3 这种存储方式实际上就是使用另外一个表去表示关联关系。fid是当前父节点的id，sid则是子节点的id，换言之每个节点都会关联上它所有的子节点。 所以查询子节点和父节点的方式也是一目了然： # 查询相关子节点 select * from table_level where id in (select sid in table_relation where fid = 1); # 查询相关父节点 select * from table_level where id in (select fid in table_relation where sid = 3); 插入新的节点呢，节点信息表就直接新增一条记录并获取到id（例如id为4），然后作为level3的子节点： INSERT INTO table_path (fid, sid) SELECT t.fid, 4 FROM table_path AS t WHERE t.sid = 3 union all select 4,4; 删除节点以及删除子树也非常的简单： # 删除节点 delete from table_path where sid = 3; # 删除子树 delete from table_path where sid in (select sid from table_path where fid = 2); Closure Table 是最通用的设计，也是唯一允许一个节点属于多个树的设计。 它需要一个附加表来存储关系，此设计在对深层次进行编码时还使用了大量行，因此增加了空间消耗。 总结]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目录树的设计与实现]]></title>
    <url>%2F2019%2F09%2F27%2F%E7%9B%AE%E5%BD%95%E6%A0%91%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[上次一更新已经是六月底了，最近工作比较忙而且有变动，一眨眼就快到国庆了，趁着这个小长假回顾和总结一下这段时间的工作。 最近给公司内部做了一个论坛，一期的功能非常的简单，简单到以至于某些功能不能很好的满足公司的需求，所以大佬给了二期的目标，主要就是实现多级分类。 在表设计上面，无论是多少级分类，都只需要一个parent_id指向父级id就行了，没错，我们原来的表就是这样设计的，所以暂时不需要做更改。 那么，我们需要改的只是代码的逻辑部分，虽然下达的任务是支持到四级分类，但是大家工作这么久了都肯定知道，需求是多变的，而且还是内部应用的需求，所以考虑再多一点不会有什么坏处，于是我开始了无限级分类的设计之路。 其实无限级分类的实现，也不是啥高深的东西，无非就是递归查询到最底层的节点为止，所以一开始的代码实现也是很简单粗暴。 public function getMetaTree() { return $this-&gt;recursiveGetMeta(0); } private function recursiveGetMeta(int $parentId) { $tree = []; $metas = Meta::searchForMetaByParentId($parentId); foreach ($metas as $key =&gt; $meta) { $tree[] = [ 'id' =&gt; $meta-&gt;id, 'name' =&gt; $meta-&gt;name, 'children' =&gt; $this-&gt;recursiveGetMeta($meta-&gt;id), ]; } return $tree; } 构造了一个简单的多维数组，然后就可以给前端返回Json格式的数据了。当然这样做是很简单，但是存在有不少问题，例如需要获取某分类下所有的叶子节点（没有子分类的分类），就需要自己再去实现一个方法，而查询逻辑的代码大部分是冗余的，只是对结果的筛选有细微的差异。那是不是把递归查询抽象出来就好了呢？我的个人看法是让开发对查询尽量无感知，所以我下面引入了分类树，或者也可以称为目录树，然后将需要或者将要用到的方法封装作为树的方法，来减少日后的迭代对数据库的依赖。下面就是这个目录树的基本定义： 树定义 public class MetaNodeTree { private $node; private $children = []; public function __construct(Meta $node) { $this-&gt;node = $node; } public function setChildren(MetaNodeTree $tree) { $this-&gt;children[] = $tree; } public function getNode() { return $this-&gt;node; } public function getChildren() { return $this-&gt;children; } /** * 是否零节点 * @return bool */ public function isNullNode() { return is_null($this-&gt;getNode()-&gt;id); } /** * @return int */ public function getDirectChildrenCount() { return count($this-&gt;children); } /** * @return bool */ public function hasChildren() { return $this-&gt;getDirectChildrenCount() &gt; 0; } } 非常简单的一棵树，任何的添加操作都没有进行排序，所以寻址的时间复杂度也是高达O(N)，不过没关系，我们首要目的还是先解耦，下面我们改动上面的查询代码来构造这一棵树。 构造树 public static function getMetaTree(int $parentId = 0) { if (false !== $nodeTree = Yii::$app-&gt;cache-&gt;get(&quot;keyForCache{$parentId}&quot;)) { $nodeTree = new MetaNodeTree(new Meta()); $subMetas = Meta::getMetasByParentId($parentId); foreach ($subMetas as $child) { $nodeTree-&gt;addChild(self::recursiveGetMeta($child)); } Yii::$app-&gt;cache-&gt;set(&quot;keyForCache{$parentId}&quot;, $nodeTree, $endurance); } return $nodeTree; } private static function recursiveGetMeta(Meta $meta) { $nodeTree = new MetaNodeTree($meta); foreach ($meta-&gt;children as $child) { $nodeTree-&gt;addChild(self::recursiveGetNode($child)); } return $nodeTree; } //定义一对多关系查询 public function getChildren() { return $this-&gt;hasMany(self::class, ['parent' =&gt; 'id']) -&gt;from(self::tableName()) -&gt;orderBy(['order' =&gt; SORT_ASC]); } 好嘞，这样子就构造好了一个获取MetaNodeTree的方法，下面我们来写写有什么实用的树操作。 首先树的遍历是常规操作，至于怎么遍历呢，方法有很多种，根据不同的场景需要提供不同的遍历方法，需要考虑层次优先级的就按照需求使用广度优先搜索或深度优先搜索，不需要考虑的那就用其中一种就行，但我们提供的遍历方法得到的结果集只能体现顺序，并不能体现层次，所以有特殊需要的时候，可能还是需要自己实现一个遍历方法并拿到自己需要的结果集，下面是深度和广度优先搜索的具体实现（hexo的MarkDown貌似有点问题，尖括号和&amp;&amp;会变转义，大部分地方的&amp;&amp;用and替代了，不过and的优先级比较低，源代码有的因为&amp;&amp;加了多余的括号）。 遍历树 /** * @param bool $nullable 是否获取零节点 * @return array[Meta] */ public function dfsTraverse($nullable = false) { $ret = []; $stack[] = $this; while (count($stack)) { /** @var MetaNodeTree $nodeTree */ $nodeTree = array_pop($stack); if ($nullable || !$nodeTree-&gt;isNullNode()) { $ret[] = $nodeTree-&gt;getNode(); } /** @var MetaNodeTree $child */ foreach ($nodeTree-&gt;getChildren() as $child) { $stack[] = $child; } } return $ret; } /** * @param bool $nullable 是否获取零节点 * @return array[Meta] */ public function bfsTraverse($nullable = false) { $ret = []; $queue[] = $this; while (count($queue)) { /** @var MetaNodeTree $nodeTree */ $nodeTree = array_shift($queue); if ($nullable || !$nodeTree-&gt;isNullNode()) { $ret[] = $nodeTree-&gt;getNode(); } /** @var MetaNodeTree $child */ foreach ($nodeTree-&gt;getChildren() as $child) { $queue[] = $child; } } return $ret; } 遍历暂时先提供了上述两种方法，得到的结果集就是一维的Meta对象数组，如果需要获取所有对象的属性，只需要遍历一次就行了。 除了基本的树遍历操作，获取树的叶子节点也是一种非常实用的方法。 获取树叶子节点 /** * 获取所有叶子节点对象 * @return array[Meta] */ public function getLeaves() { $leaves = []; $stack[] = $this; while (count($stack)) { /** @var MetaNodeTree $nodeTree*/ $nodeTree = array_pop($stack); !$nodeTree-&gt;isNullNode() and !$nodeTree-&gt;hasChildren() and $leaves[] = $nodeTree-&gt;getNode(); /** @var MetaNodeTree $child */ foreach ($nodeTree-&gt;getChildren() as $child) { $stack[] = $child; } } return $leaves; } 说它实用当然是因为我们有需要使用到的场景，由于我们的分类层级不固定，有的可能是五级，有的可能就只有三级，而我们的文章只能关联叶子节点的分类，所以以上方法可以帮助我们获取一个大分类下面所有叶子节点的id，进而获取到所有相关联的文章。 目录树的常用方法大概就是以上这些了，现在我们回到构造树的地方，看看这棵树在构造的时候存在的一些问题。 首先大家可以看到这棵树的构造需要传参parentId，这就意味着不一样的parentId代表着不一样的树，那么如果分类节点是 1000 个，那么就有可能构造出 1000 颗树并缓存起来，由于缓存的是序列化的树对象，占用空间会比较大，如果树的数量更多的话，很可能会把缓存服务打垮。 所以下面我们试着来改一下构造这棵树的方法，目的是尽量只缓存一棵树。 构造的优化 public static function getMetaTree(int $parentId = 0) { if (false !== $nodeTree = Yii::$app-&gt;cache-&gt;get(&quot;keyForCache&quot;)) { $nodeTree = new MetaNodeTree(new Meta()); $subMetas = Meta::getMetasByParentId($parentId); foreach ($subMetas as $child) { $nodeTree-&gt;addChild(self::recursiveGetMeta($child)); } Yii::$app-&gt;cache-&gt;set(&quot;keyForCache&quot;, $nodeTree, $endurance); } if ($parentId !== 0) { $nodeTree = $nodeTree-&gt;dfsGetSubTree($parentId); } return $nodeTree; } 这里我们只缓存从根节点开始的完整的一棵树，若入参非 0 ，则通遍历树找到对应的子树，下面是找子树的方法。 dfs搜索子树 public function dfsGetSubTree(int $parentId) { $stack[] = $this; while (count($stack)) { /** @var MetaNodeTree $nodeTree */ $nodeTree = array_pop($stack); if ($nodeTree-&gt;getNode()-&gt;id === $parentId) { return $nodeTree; } /** @var MetaNodeTree $child */ foreach ($nodeTree-&gt;getChildren() as $child) { $stack[] = $child; } } return $this; } 通过遍历树解决了内存占用过多的问题，其实就是时间换空间的思想，不过这棵树的搜索时间复杂度是O(N)，还是有很大的优化空间，不过在这之前还有一个需要解决的问题，那就是并发的时候在构造树的地方会有多个实例访问到db，这里我们再给他加上一个锁，如果是单节点的用语言级别的锁即可，不过建议还是上分布式锁，即使现在是单节点部署，分布式锁还是可以兼容日后的架构变更。 并发优化 public static function getMetaTree(int $parentId = 0) { $cache = Yii::$app-&gt;cache; if (false !== $nodeTree = $cache-&gt;get(&quot;keyForCache&quot;) and self::distributeLock()) { $nodeTree = new MetaNodeTree(new Meta()); $subMetas = Meta::getMetasByParentId($parentId); foreach ($subMetas as $child) { $nodeTree-&gt;addChild(self::recursiveGetMeta($child)); } $cache-&gt;set(&quot;keyForCache&quot;, $nodeTree, $endurance); self::distributeUnlock(); } (!$nodeTree instanceof MetaNodeTree) and (!($nodeTree = $cache-&gt;getIfExist($cacheKey)) instanceof MetaNodeTree) and $nodeTree = new MetaNodeTree(new PostMeta()); if ($parentId !== 0) { $nodeTree = $nodeTree-&gt;dfsGetSubTree($parentId); } return $nodeTree; } 上面的分布式锁就靠大家自己实现了，需要注意的是getIfExist($key)这个函数，主要是因为$cache-&gt;get($key)获取的是有效的缓存，若缓存依赖的db数据模型有更新get($key)函数遍会获取到false，实际上键值还是没有删除的，这里其实只要自己按照$key去直接获取旧的缓存内容就行，毕竟我觉得这个场景下旧的缓存数据总比空数据要显得正常得多。 不知不觉就到了国庆的最后一天了，我好像过了一个假的国庆，监狱这是今年最后一个假期，那么这里只能预祝大家新年快乐了。😂 完。]]></content>
      <categories>
        <category>思考</category>
      </categories>
      <tags>
        <tag>design</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道简单的动态规划题目]]></title>
    <url>%2F2019%2F06%2F30%2F%E4%B8%80%E9%81%93%E7%AE%80%E5%8D%95%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[上一篇SQL的坑挖得有点深，还没有太大的进展，不过也在进行调试和看源码，闲暇时间做了一些简单的题目，分享一下心路历程。 题目是这样的： Given an integer array with all positive numbers and no duplicates, find the number of possible combinations that add up to a positive integer target. # Example nums = [1, 2, 3] target = 4 The possible combination ways are: (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) Note that different sequences are counted as different combinations. Therefore the output is 7. 题意的主要就是说给定一个目标数和一个数组，计算出这个数组中能组成该目标数的所有路径之和。 那么简单分析，我们用倒推的思路，4减去数组中的三个数，就有三条路径，得出三个数分别是3，2，1；接着重复上面的步骤，3又有三条路径，得出2，1，0；我们发现这个时候有一条路已经达到了0，就是说这是一条可以完整组成4的路径，所以这条路径已经完成了，不需要再往下走；换言之，就是当目标数被减至0，就应该是我们在函数里面的计数和退出条件。所以递归的解法可以很快就写出来了： class Solution: def combinationSum4(self, nums: List[int], target: int) -&gt; int: ways = 0 caTarget = target for num in nums: target = caTarget target -= num if target == 0: ways += 1 if target &gt; 0: ways += self.combinationSum4(nums, target) return ways 简单的测试了几个案例，都通过了，不过呢，将目标数调大之后，发现就超时了，意料之中，毕竟递归栈随着目标数的变大变得非常深，而且每条路径都要完整的走完，简单的计算一下时间的复杂度，假如目标数除以数组平均值等于k，数组长度是n，那么时间复杂度大概是O(n^k)。 当然，上述递归解决方案并不完善，于是我就想可以先减少每次传递的数组长度来优化，所以做了简单的“优化”： class Solution: def combinationSum4(self, nums: List[int], target: int) -&gt; int: ways = 0 caTarget = target for num in nums: target = caTarget target -= num if target == 0: ways += 1 if target &gt; 0: newNums = [] for iNum in nums: iNum &lt;= target and newNums.append(iNum) if len(newNums) == 0: continue ways += self.combinationSum4(newNums, target) return ways 为什么“优化”两字有双引号，我想大家都明白，很多时候是为了表达一种失败的意思，这里的“优化”也不例外。 至于原因嘛，想一想其实也很容易明白，当目标足够大的时候，传递的数组只在最后几步的时候才会减少长度，基本起不到优化的作用，而且，还需要另外创建很多的数组进行赋值操作，耗费内存之余，也耗费了很多时间。 那么我们该从何优化呢？其实我们一开始在倒推的时候，有的人可能已经发现了我们列出来的路径中，存在了很多重复的步子。例如走到4的路分别会经过3，2，1，走到3的路会分别经过2，1，那么说明在走到3的时候就已经计算过2，1的路径数了。现在换种方式表达，有一个f函数，用f(4)表示到4的路径数，f(4)=f(3)+f(2)+f(1)，再如上面解法递归下去，就是列出了所有的路径。到达f(4)的途中，有很多个f(3)，f(2)，f(1)，那我们只需要在第一次计算得到它们的值之后，记录起来，那么在后面的计算中，它们再出现，我们就能直接拿到值，不需要把整个路径再重新走一遍。对应的代码表达如下： class Solution: globalTarget = {} def combinationSum4(self, nums: List[int], target: int) -&gt; int: ways = 0 caTarget = target if caTarget in self.globalTarget: return self.globalTarget[caTarget] for num in nums: target = caTarget target -= num if target == 0: ways += 1 if target &gt; 0: ways += self.combinationSum4(nums, target) self.globalTarget[caTarget] = ways return ways 这个优化方案是没什么问题，理论上能通过测试，不过你拿这个python代码去leetcode贴也是不行的，这种使用公共全局变量的方式在leetcode提交会有问题，于是我换了个语言声明了私有变量，测试通过。 不过呢，上面只是对递归解法的优化方案，这个好像跟题目所提及的动态规划没有联系上，我是标题党吗？ 对！这周先当个标题党吧，下周再补充。。。也希望早日把上一篇的坑填完。 一下子鸽了两周了，工作和生活都很繁忙。SQL的填坑恐怕遥遥无期，不过简单的东西还是可以先说完的。 之前的分析其实就代原理的理解来说，已经很接近动态规划了，只不过思想方面，两者是截然相反的。上面的代码是处于一种倒推的思想，而动态规划呢，则是从头开始一步一步的算到最后的目标。 如果不理解的话，我把上面的例子再用动态规划的思想说一次。上面的递归我们是从f(4)一直倒推下去的，现在我们反过来，从f(0)一直算到f(4)，理解了递归的优化算法，那么理解这个也很简单。 class Solution(object): def combinationSum4(self, nums, target): dp = [0] * (target + 1) dp[0] = 1 for i in range(target + 1): for num in nums: if i &gt;= num: dp[i] += dp[i-num] return dp[target] 代码就是如上了，如果看我说的看不懂，可以慢慢理解一下代码，其实跟递归的思考方式相反，但是递归实际上也是从最深处开始计算的，我认为最大的差异还是递归能够只计算能够到达的路径，而动态规划是计算了才知道这一步能不能到~ 完。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次风骚的SQL调优]]></title>
    <url>%2F2019%2F05%2F26%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E9%A3%8E%E9%AA%9A%E7%9A%84SQL%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[这次SQL调优的事情已经过去一段时间了，一直都没有时间彻底搞懂那个慢查的原因，先mark下来再慢慢研究。 具体事件起因经过就忽略吧，这里就简单的介绍一下基本信息以及模拟一下表结构和慢查询的语句。 当时使用的MySQL版本是5.7，具体的小版本大家就别问了，问我也不记得了。 然后表结构最主要的部分就如下所示： # father表 CREATE TABLE `father` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `son_id` int(10) unsigned NOT NULL, `fname` varchar(255) NOT NULL DEFAULT '', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; # son表 CREATE TABLE `son` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 当时的慢SQL其实也非常的简单，大概就是这个样子的： select count(0) FROM test_father WHERE son_id in (SELECT id from test_son); 当时的数据量并不是很大，father表符合条件的数据大概是两到三万，son表总数据不到一万，最后查询得到的结果是6000左右，即是大概有6000条记录满足条件，但是查询时间大概要6秒，我在自己电脑上面大概模拟了一下数据量，也出现了慢查的情况，不过由于减少了其他的列或者总数据量有差别的原因，总的查询时间大概是4秒多，而我的MySQL版本是5.7.16。 当然，father表最大的问题就是son_id字段没有建立索引，但是这数据量也并不大啊，MySQL应该几十毫秒就能搞定的吧~ 什么，你不信？我给大家看看另外一个查询哈： select count(0) FROM test_father WHERE son_id NOT in (SELECT id from test_son); 不是说反向查询对索引的利用并不友好的吗，怎么在没有索引的情况下NOT IN比IN快这么多？ 一开始我尝试使用explain查看过第一个语句，发现查询语句MySQL优化成了半连接查询，关于连接查询，大家都知道如果能利用索引的话，耗费的时间大概是A表满足条件的记录数乘以B表索引的搜索时间（基本可以认为是常数时间），而A表是哪张表，是由满足条件的记录数决定的，记录数越少的，就会被上拉成为A表把数据加载到内存中，然后再拿出每条数据与B表关联的列数据去B表查询，因为索引的关系，每条记录查询的次数一般不会超过4次。 而如果是不能利用索引的情况呢，A表还是那张记录数少的表，但是B表由于不能利用到索引，所以是需要全表遍历的，而MySQL为了减少IO的次数，会有一个join buffer的数据缓存区缓存B表的数据，当然如果你设置的join buffer大小太小以至于不能完全装下B表所有数据的话，跟每条A表数据比对的时候都要轮流替换join buffer中B表的数据。 那时候的我就以为这个慢查询的主要原因就在于MySQL将它优化成了半连接查询，而我们的father表又没有建立好索引，而且当时数据库join buffer的参数设置也只是使用了默认值，所以导致这个SQL执行非常耗时。 由于各种条件限制，不能立刻给线上数据库加索引或者修改join buffer参数大小，于是乎当时的想法就是想办法让MySQL不将这个查询优化为连接查询，而最简单又容易转化数据的办法就是反向查询了，也就是图二中展示耗时0秒的查询。 我后来也用explain查看了NOT IN子查询的执行过程，但是没有太大的收获，只能看得出它确实是直接执行了子查询，也就是没有优化成连接查询： 后面使用了的optimizer_tracer观察了它的优化流程，也没能很好的理解这个NOT IN之所以“快”的原因。后来在一些学习资料上面阅读到了在一些情况下，MySQL会建立不同类型的临时表（依据数据量和参数设置）并自动建立索引这样的机制。 所以后来天真的我得出的结论就是，上面的NOT IN语句MySQL会给father表建立临时表并对son_id建立索引，所以使得IO次数大概是 son表记录数乘以常数 ，大概是连接查询的 son表记录数乘以father表记录数 的万分之一左右（两个表满足条件的记录数都是五位数），考虑到join buffer的因素，实际IO次数比应该是百分之一到千分之一，也就是上面NOT IN比IN快数百倍的原因。 但是真正的原因真的是这样吗？过了一段时间我又在别的电脑模拟了当时的情景，由于已经过去比较久了，流程记得不太清楚，漏了某些我认为不重要的步骤，结果得出来也十分的意外，相仿的数据量，相似的表结构，简化了的语句，并没有构成慢查询，无论是IN还是NOT IN，查询时间都是毫秒级别。 这就引起了我强烈的好奇心，不断的复盘，想起了一些自己漏掉的步骤： son表并不是一开始就只有一万的数据，而是导入了第三方的百万条数据 由于需求变更，son表只保留了最新的一万条左右的数据，其余的数据全被delete掉了 整理了漏掉的这些步骤之后，我再重新模拟了多遍当时的场景，终于重现了慢查询，而且发现了son表中的一些异常，而我认为这些异常正是导致慢查询的真凶。 大家可以看看关于son表的一些统计数据： 表里面明明有一万条记录，但是却行数是0；father表记录和数据都比son表多不少，但是father表的Data Length却只有1M多。虽说innodb是不准确的统计，但也没有这么这么夸张吧，到底是什么原因导致统计到0行数据以及数据长度差异如此大呢，我们可能要深入探索一下innodb的数据统计方法~ 首先，innodb的记录数的计算，是根据算法选取STATS_SAMPLE_PAGES个叶子节点页面，然后计算得到选取页面中主键值的平均数，再乘以全部叶子节点数，最后就得出了该表的总记录数。 那么实际上这个计算的方法排除掉设置的那些参数，剩下的变量就只有两个了，其中一个是叶子结点数，另外一个就是选取的页面中的主键平均数。所以导致最后统计的结果为0的因素就基本确定了，要么叶子结点数为0，要么主键平均数为0。叶子节点数为0，这个就不需要讨论了，因为没有叶子节点就是一个数据页都没有，根本就没数据，不会跟实际记录产生出入，所以最终的元凶就是选取页面中主键值的平均数了。 相信有的童鞋比较熟悉innodb的数据页管理，那么你们肯定知道innodb删除数据的时候并不会从数据页中抹除，只是会标记为已删除。所以son表的统计记录数为0的原因就是因为删除了99%的数据，而统计的时候又刚好选取了记录全部都被标记为已删除的数据页，得出的平均数自然也为0了。 关于Data Length的统计问题我还没搞懂，不过既然已经知道记录数统计误差的来历了，我们也可以先回到一开始的问题。所以这个统计的数据到底是否影响到了查询的效率呢？我可以一脸认真的告诉你，我现在也不知道，不过我想知道，如果你也想知道的话，那咱们再查查资料，或者看看执行过程和代码？]]></content>
      <categories>
        <category>有趣</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次面试引发的自闭]]></title>
    <url>%2F2019%2F04%2F12%2F%E4%B8%80%E6%AC%A1%E9%9D%A2%E8%AF%95%E5%BC%95%E5%8F%91%E7%9A%84%E8%87%AA%E9%97%AD%2F</url>
    <content type="text"><![CDATA[金三银四到了，我想各位互联网的童鞋们都有想到要去历练（mianshi）一番的想法。因为历练能给到你很多东西，例如能帮助你更加了解自身，能帮助你开拓视野，说不定还能帮你登上一个台阶。但是我却没想到，经历了一次面试之后，我自闭了。 事情是这样的，我投了某家独角兽公司，一面是视频面试，考核的知识点不多也不深，偏重解决方案与代码实战，整个过程感觉还是较为轻松的，拿到了二面的机会。 二面当天狂风暴雨，赶了几个小时路终于到了，下半身都基本湿透。等待了一阵子面试官就招呼我过去面试了，过程就不详细说了，反正我愣是一道题都没做对😂。两个小时的面试，基本在解决逻辑和算术问题，不能只说想法，需要公式和计算结果来体现。虽然我觉得我每道题目的思路都是没有太大问题的，但是最大的问题就是我在用公式表达和计算出来的答案跟我想的都不一样，当场怀疑人生，万念俱灰。不过还是很感谢面试官没有提早放弃我，只怪自己的能力有所欠缺。 故事概要就说到这里了，写这篇博客的目的还是对这次面试中没有好思路的一个算法题目进行解析。原题目如下： 给定一个数组，例如：[21, 11, 34, 42, 31, 66, 78, 70, 89, 121, 111] 找出数组中所有满足比前面所有元素都大并且比后面所有元素都小的元素的位置。 要求时间复杂度和空间复杂度都为 O(N) 。 如果不考虑时间复杂度，那么最符合人类思考方式的就是每个元素都分别跟它前面所有和后面所有的元素进行对比，这种应该就是最直观的解决方案，但是问题就在于，这种解决方案的说白了就是两层嵌套循环的遍历数组元素，那么这样一来时间复杂度就达到了 O(N^2) 的级别。 那要怎么才能做到题目要求的那样，时间复杂度和空间复杂度都为O(N)呢？面试官给了我如下提示： 遍历一次和遍历两次，时间复杂度都是O(N)。 多几个和这样长度一样的数组，空间复杂度也还是O(N)。 比前面所有都大，换句话来说是什么？ 这……这我都知道啊，慢着，最后那句啥意思？到最后我都没有想明白最后那个提示是啥意思，是的，现在也没想明白。但是想不明白也要解决，既然想不明白面试官的提示，那就自己另辟蹊径呗。 这题目其实一开始我理解错了，因为原题里面是没有所有这个字眼的，跟面试官确认之后发现题意是如上所述那样，但是我想了一下，先确认每个元素是否跟前后两个元素满足上述条件，然后再对每次得到的结果集重复上述步骤直到结果集元素数量不再减少，也不妨是一种方案，再往深一想，这两种描述所得到的最后结果其实都是已经排好序的一个数组。但实际上这样的思路是有问题的，不过我们还是看一下代码，再进行分析： ############## 错误示例 ############### $array = [21, 11, 34, 42, 31, 66, 78, 70, 89, 121, 111]; function getResult(array $arr) { $lastCount = 0; while(count($arr) != $lastCount) { $lastKey = -1; $lastNum = -INF; $lastCount = count($arr); foreach ($arr as $key =&gt; $value) { if ($value &lt;= $lastNum) { unset($arr[$key]); if ($lastKey &gt; -1) { unset($arr[$lastKey]); } } $lastKey = $key; $lastNum = $value; } } return $arr; } print_r(getResult($array)); 大家可以看到我们这部分的代码是嵌套循环的，当外部循环发现元素数量不再改变的时候，就跳出循环，返回结果。那么这样的一个计算方法的时间复杂度和空间复杂度是在哪个级别呢，我们来慢慢分析一下。 首先，最好的情况，数组内所有的元素都满足题目要求，则外层循环只执行一次，里层遍历数组一次，所以最理想的情况的时间复杂度是O(N)。接下来我们想想一些极端情况，例如除了最后一个元素，前面所有的元素都是排好序的，但是偏偏最后一个元素就是最小的，这种情况下，最后一个元素会先被消除掉，所以后面再比较就都符合题意了，所以这种解法实际上是有问题的，我们给代码标注上错误示例的标识。 既然上面的方案有问题，那我们还需要重新整理思路。我们刚刚想要的是最后的结果是一个符合题目要求的元素的排好序的数组，那么原来的数组我们可以看作是乱序的数组（不排除有序的可能），而符合题目要求的元素在原数组里面是处于什么位置呢？其实我们可以尝试将原数组进行排序，大家会发现符合题意的元素位置跟排序前是没有变化的，因为前面比它小的元素排序不会排到它后面去，后面比它大的元素也同理。 于是我们的解决方案就可以是：对原数组进行排序，然后再对比排序前和后的数组，两数组同下标情况下元素数值一样的，就是符合题意的元素了。那么开始写代码： ########### 时间复杂度和空间复杂度在某些情况下能满足题目要求 ########### $array = [21, 11, 34, 42, 31, 66, 78, 70, 89, 121, 111]; function getResult(array $arr) { $map = []; $sortedArr = $arr; sort($sortedArr); $result = []; for ($i=0; $i &lt; count($arr); $i++) { $map[$arr[$i]] = isset($map[$arr[$i]])?$map[$arr[$i]]+1:1; if ($arr[$i] == $sortedArr[$i]) { $result[] = $arr[$i]; } } foreach ($result as $key =&gt; $value) { if ($map[$value] &gt; 1){ unset($result[$key]); } } return $result; } print_r(getResult($array)); 同样的我们先计算一下时间复杂度，这个方案的耗时主要是排序和遍历部分（后面加入的排除相同数的部分代码对两种复杂度都没影响）。遍历部分只遍历一次，看作O(N)，那么排序的耗时，主要还是看排序算法，较为通用的排序算法时间复杂度一般都是O(NlogN)到O(N^2)之间，好像不满足。那么我们就暂时假定可以使用限制性较大的排序，如计数排序，那么这个方案的时间复杂度就是O(N)了。 接下来再看看空间复杂度。方案内使用的额外空间包括排序算法所需要的额外空间，以及我们复制出来的待排序数组和定义的$result。上面假定可以使用计数排序，空间复杂度跟最大值有关，那么这样看来空间复杂度有点不可控。 这个方案理论上没有太大问题，如果说有相同的数那么加多一个Map进行排除掉就行了。但是数组元素的取值必须在限制性较大的范围内才能满足题目对时间复杂度和空间复杂度都的要求。那么还有没有更加好的思路呢？ 有是必须有的，实际上更好的解决思路就是面试官给予的提示，到现在我才恍然大悟，他所给的三个提示，可以说的就是这道题目的简化版的答案。这个思路最早在前面两个方案想出来之前已经走过一遍了，但是脑子转不过弯，只解决了一半，往下好像还缺点什么，就当作是错误的思路抛弃了。 一开始我想的是：初始化最大值为负无穷，遍历数组，将当前元素与最大值比对，得出最大值并替换，将最大值的放到数组2中，直到遍历完成。然后原数组与数组2比对，对应下标的元素如果值相等，那就是满足比前面所有元素都大的这个条件了。但是接下来的问题是那怎么满足比后面都小这个条件呢？虽然很不想承认，但是我真的是名副其实的猪脑子😂。那时候我想的是： 那不是又要访问当前元素的后面一个元素是不是跟当前元素一样，如果一样就是不满足，那再后面一个元素如果又替换了我怎么知道原来的值？ 大家不要被我带偏了，现在想来是真的蠢，都已经会从前面遍历将大值覆盖小值了，怎么就不会从后面再遍历一次将小值去覆盖大值呢？所以解决方案真的就是那么简单： ##############更好的解法，这里用map去除重复值就略了，参照上面的就行############## $array = [21, 11, 34, 42, 31, 66, 78, 70, 89, 121, 111]; function getResult(array $arr) { $max = -INF; $arr2 = []; $min = INF; $arr3 = []; $result = []; for ($i=0; $i &lt; count($arr); $i++) { $max = $max &gt; $arr[$i] ? $max : $arr[$i]; $arr2[] = $max; } for ($j=count($arr)-1; $j &gt;= 0; $j--) { $min = $min &lt; $arr[$j] ? $min : $arr[$j]; $arr3[$j] = $min; } foreach ($arr as $key =&gt; $value) { // 不知道为啥它把我代码里面的 &amp;&amp; 转义了-_-#，那我就分开写吧。这注释你就不转义-_-#，存心逗我吧 if ($value == $arr2[$key]){ if ($value == $arr3[$key]) { $result[] = $value; } } } return $result; } print_r(getResult($array)); 虽然说面试现场紧张也是在所难免，但是马后炮就更加没意思了。失败不是偶然，都是自身能力不足导致的，所以最重要的还是从失败中吸取教训，面试中的每一个问题，即使当场提供了解决方案的，都可以再深入研究。至于紧张情绪方面的，也不是说面多几次就能适应，还是需要对自己有充足的信心，也需要明白面试官不是你的敌人，他们都想早日找到合适的人，所以他们也是会帮助你的。 最后，其实上面都是我自己写给自己看的😄，相信各位童鞋都早已解决了上述的问题，相信也有很多的童鞋已经拿到了心仪的offer了。但是不管捷报与否，生命不止，奋斗不息。 完。]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的会写冒泡排序吗？]]></title>
    <url>%2F2019%2F03%2F14%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E4%BC%9A%E5%86%99%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[大多数人接触到的第一种排序算法恐怕非冒泡排序莫属，它的排序思想十分的简单直观，相信大家都能用很简短的代码来实现冒泡排序，但是往往越简单的东西，能玩得花样就越多，今天我就来抛砖引玉，希望大家能给出会72变的冒泡排序。 废话就不多说了，简单的冒泡排序代码敬上： &lt;?php $arr = [7, 6, 8, 9, 0, 1, 5, 4, 3, 2]; $n = count($arr); for ($j = 1; $j &lt; $n; $j++) { for ($i = 0; $i &lt; $n - $j; $i++) { if ($arr[$i + 1] &lt; $arr[$i]) { $arr[$i] += $arr[$i + 1]; $arr[$i + 1] = $arr[$i] - $arr[$i + 1]; $arr[$i] = $arr[$i] - $arr[$i + 1]; } } } print_r($arr); OK，代码就这么简单，高高兴兴的写完了，这时候面试官问：代码还有优化空间吗？ 难道面试官看到了代码有bug？还是说字太丑了？如果你这时候说：那必须的，我给你换个静态编译的语言写一遍！ 那恐怕这道题目就到此为止了。面试官真正的目的是什么，其实大家都心知肚明，只是在面试过程中由于紧张等情绪影响，会错失一些加分的机会，所以我们平时就要学会多思考，再简单的知识点我们都可以往深挖掘，培养好习惯，那下次遇到一些开放式题目的时候，就算想不出，心里也能坦荡荡。😂 扯了那么多，赶紧回到正题。我们可以看到上面的代码，实在太简单了，简单到你只要你的待排序数组元素数量一样，不管你的待排序数组内容如何，甚至已经是完全排好序的，它的遍历的次数也是固定的。这就十分的浪费资源了，所以我们得想办法让它对已经排好序的数组停止遍历，放开那个数组，冲我来！ 没错，我们要做的就是半路叫停它，教育它，让它迷途知返。我们都很清楚冒泡排序它的特点，那就是元素交换，既然我们都抓住了它的把柄了，教育便是水到渠成的事情了。 &lt;?php $arr = [7, 6, 8, 9, 0, 1, 5, 4, 3, 2]; $n = count($arr); for ($j = 1; $j &lt; $n; $j++) { $changeFlag = false; for ($i = 0; $i &lt; $n - $j; $i++) { if ($arr[$i + 1] &lt; $arr[$i]) { $changeFlag = true; $arr[$i] += $arr[$i + 1]; $arr[$i + 1] = $arr[$i] - $arr[$i + 1]; $arr[$i] = $arr[$i] - $arr[$i + 1]; } } if(!$changeFlag) { break; } } print_r($arr); 其实就是这么简单，给它立个flag，要是flag达成了，那好说，你可以继续；要是没达成，那就乖乖给俺放弃，不要执迷不悟了，勉强是没有幸福的。 优化了代码之后，你感觉自己像成功的教育了一个孩子一样开心无比，刚好面试官又在你对面，你就可以跟面试官倾诉，你是如何含辛茹苦的将这个孩子养育成人的。然而面试官却说：我觉得还有优化空间欸，你觉得呢？ 你就想，那当然啊，孩子能考上大学，谁也不想高中就辍学啊。可是家庭条件不允许啊，面试官你要体谅我啊。 心里虽然这么想，但是嘴巴却很巴结：嗯！我也是这么觉得的。面试官：那……。你：那容我先解决一下内急！ 接下来就是东拼西凑的时候了，必须要借够钱让孩子上大学！当然，网贷这种东西，最好别碰，出了事自己都搭进去了。所谓的东拼西凑，当然是指努力寻找你脑海中的记忆碎片，说不定还能找到新大陆。但是在找到新大陆前，你最好还是先想想有没有老办法可以解决这个问题。你一脸懵逼：老办法？ 没错啊，不是刚刚才写完了吗？你这么快就忘了你怎么含辛茹苦的将它养育成人吗？ 此时你的脑海中： 你的记忆是鱼的记忆吗？突然你恍然大悟，说道：你的意思是面试官可能跟鱼一样只有7秒记忆，就算我把刚刚的代码再写一遍，面试官也看不出来？ 醒醒，你再不出去面试官就要来厕所找你了。你心里着急道：可是还没想还有啥办法啊！ 老办法是不能再写一遍，但是可以按老思路再将老办法优化一下啊。你回想一下刚刚的老办法是等到某次遍历之后的发现完全没有flag达成才退出循环，但是过程中你不知道它到底在哪些地方达成了flag。一旦你能够得知它在哪些地方达成了flag，那么就可以让它避免弯路，直接踏上通往成功的捷径了。 你回到了面试间，心里嘀咕着些什么，抓起了笔开始写代码，这次你特意将字写得特别大，不知道是为了掩饰内心的捉急，还是为了掩饰特别宽的行距。 面试官似乎看出了什么，说道：用我的手提电脑吧，IDE、终端随便用，别给我百度就行。 表示感激之后，你立马将上一个优化版本的代码写到IDE里面，随后便是echo、print_r等到处飞。很快你就摸索出来了一些门路，发现有几次循环正在排序的数组内只有最前面的几个元素需要比较排序，后面大部分的元素已经是有序的了，但是结束比较排序的下标却将后面已经排好序的元素也囊括在内了。你会心一笑，暗道原来捷径就在这里。 &lt;?php $arr = [6, 3, 54, 6, 23, 4, 26, 7, 18, 1, 4, 5, 67, 48, 9]; shuffle($arr); $n = count($arr); $lastChangePos = $n - 1; for($j = 1; $j &lt; $n; $j++){ $changeFlag = false; for($i = 0; $i &lt; $lastChangePos; $i++) { if($arr[$i] &gt; $arr[$i+1]) { $changeFlag = $i; $arr[$i] += $arr[$i+1]; $arr[$i+1] = $arr[$i] - $arr[$i+1]; $arr[$i] = $arr[$i] - $arr[$i+1]; } } $lastChangePos = $changeFlag; if(!$lastChangePos) { break; } } print_r($arr); 写完代码，感觉到行云流水一气呵成的快感，正准备给面试官讲解，突然你心里咯噔一下：咦！万一前面乱序的比后面有序的元素大，那不是漏掉了？ 尽管心里一直在喊冷静，但你依然十分紧张，此时你发现面试官脸上出现了笑容：嗯，不错，这道题目就先这样吧，我们继续下面的问题…… 面试完后，你简单的回想刚刚的问题，才发现是自己吓自己，即使前面乱序的部分有比后面有序的部分大的元素，但是上一次循环最后交换的位置肯定在乱序元素之后，因为冒泡排序就是先将乱序部分中最大（小）的元素先给顶置上去的，所以下一次循环的时候乱序中的大元素就会跟有序中的小元素进行比较排序了。 回家后的你觉得意犹未尽，小小的冒泡排序到底还能玩出什么花样？于是你又抓起了笔，当然，这些都是后话了……]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Sorting algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你可能对MySQL的Repeatable read有所误解]]></title>
    <url>%2F2019%2F02%2F20%2F%E4%BD%A0%E5%8F%AF%E8%83%BD%E5%AF%B9MySQL%E7%9A%84Repeatable-read%E6%9C%89%E6%89%80%E8%AF%AF%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[有些东西，你深究下去，才会发现原来你的认知一直是错的。 相信大家对MySQL事务的隔离级别都不陌生： 读未提交 (Read uncommitted) 读已提交 (Read committed) 可重复读 (Repeatable read) 串行化 (Serializable) MySQL默认的事务隔离级别为可重复读 (Repeatable read)。 也许大家都见过这个在各种博客或文章里出现频率极高的表格： 事务隔离级别 脏读 不可重复读 幻读 读未提交 是 是 是 读已提交 否 是 是 可重复读 否 否 是 串行化 否 否 否 也许大家看到的样式不是这样，但是我肯定它们的灵魂都是一样的。当然上面这个表格是对的，它描述的是SQL的标准，只是在MySQL的MVCC机制下，会让人容易出现理解上面的误差。 上面确实都是废话，这篇文章就一个重点，那就是想说明MySQL在可重复读这个隔离级别下，让幻读出现所必需要的条件。我们在准确理解了MySQL可重复读之后，就可以更好的优化我们的SQL代码了。 如果有的童鞋早就知道了，请不要大喊出来，我想你们也不忍心让我知道自己到底愚蠢了多久。😂 下面我们实验完之后，如果有童鞋觉得我的观点是错误的话，那大家一起来快活的讨论吧。 由于不可重复读和幻读的概念很容易搞混，所以实验之前我们先回顾一下： 不可重复读： 多次读取一条记录, 发现其中的数据与事务内上一次的读取不一致，主要侧重其它事务的更新影响了本事务的读取内容。 幻读： 多次读取一个范围内的记录, 发现结果不一致，主要针对的是其它事务的插入和删除操作。 我要建的表结构如下： CREATE TABLE `test` ( `id` int(11) NOT NULL, `content` varchar(255) NOT NULL DEFAULT '', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 首先连接上你的数据库，查看当前的隔离级别： SELECT @@tx_isolation; 确定是REPEATABLE-READ之后，我们再新开一个窗口建立另一个连接。然后在两个窗口分别运行： -- 开启事务A START TRANSACTION; -- 开启事务B START TRANSACTION; 都开启了事务后，咱们在窗口A查询： -- 事务A SELECT * FROM test; 结果集当然为空，因为咱们表还没有任何的数据。 接下来我们就要去窗口B进行插入数据操作并提交事务： -- 事务B INSERT INTO test(content) VALUES('B插入内容1'); COMMIT; -- 事务B结束 然后再返回窗口A重新运行： -- 事务A SELECT * FROM test; 依然是没有任何数据，接着我们对A的事务进行提交并重新查询： -- 事务A COMMIT; -- 事务A已结束 SELECT * FROM test; 终于在窗口A的结果集中出现了在事务B中插入的内容： id content 1 B插入内容1 在上面这个栗子中并没有出现幻读，直接的原因就是可重复读中读取的是MVCC机制所提供的事务级的快照，所以别的事务对数据的修改并不会直接影响到当前事务的读取的数据。 然而经过我的细心研究，发现在可重复读级别下，幻读终究还是会出现的，它只不过是有点害羞罢了。 相信大家还记得当前表中的数据，如果不记得了，我们来查一下： -- 开启事务A START TRANSACTION; SELECT * FROM test; 得到结果集： id content 1 B插入内容1 接下来到新的窗口开启事务B： -- 开启事务B START TRANSACTION; INSERT INTO test(content) VALUES('B插入内容2'); COMMIT; -- 事务B结束 返回到事务A我们重新查询： -- 事务A SELECT * FROM test; 得到结果集没有改变： id content 1 B插入内容1 接下来我们来一波骚操作： -- 事务A UPDATE test SET content='A更新内容'; 执行之后，清楚的看到了反馈： [SQL]UPDATE test SET content='A更新内容'; Affected rows: 2 Time: 0.000s 我刚刚查到表只有一行数据，怎么我更新了两行？那我们再查查看： -- 事务A SELECT * FROM test; 咦？得到的结果集有两条记录： id content 1 A更新内容 2 A更新内容 相信各位聪明的童鞋读到这里都知道是什么回事了，毕竟我们上帝视角看着事务A和B，都看到这里了还装什么傻。没错，使可重复读级别下出现幻读的必要条件，就是两个事务中都有DML操作，而且是恰到好处的DML操作，否则你们的修改的内容和条件（可以理解为修改的数据所属的范围）没有一点交集的话，幻读也是不会出现的。 总的来说，在事务A中也加入DML操作的话，会发生明明能读取到，却操作不到的奇怪现象，或者会出现读取的数据中没这条记录，但是最后DML操作的时候却把这条记录也一起修改了的情况。我认为Repeatable read虽然是能在DQL操作中防止了幻读和不可重复读的情况出现，但是却在DML操作中会引入其他一些奇怪的现象。 有的童鞋看到这里可能会有一种被标题党骗了的感觉，这不是说了跟没说一样嘛，可重复读级别还是会有幻读出现啊，到最后我们还不是需要用串行化？ 但是这篇文章真的没有意义吗？在你的项目里面，为了避免幻读，是直接使用串行化？还是依然使用可重复读，只是需要在那些可能会产生幻读的地方加上GAP锁？ 如果你的回答是“都可以，怎么方便怎么来”的话，那我觉得，确实没意义。 -- 事务A COMMIT; -- 事务A结束 完。]]></content>
      <categories>
        <category>探究</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HybridApp历险记]]></title>
    <url>%2F2019%2F01%2F27%2FHybridApp%E5%8E%86%E9%99%A9%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Listen to the pain, it’s both history teacher and fortune teller. Pain teaches us who we are, Wade. Sometimes it’s so bad you feel like you’re dying but we can’t really live until we die a little, can we? 最近项目里面使用Cordova进行Hybrid App的开发，实际上我们是有一些类似于淘票票出票机那样的机器，需要一款能够在该机器的系统（安卓）中运行并调用硬件实现扫码出票等功能的安卓应用。我负责的是工作内容主要是接入供应商所提供的硬件SDK，并转化为可被Javascript调用的插件。 如何开发Cordova安卓插件，官网和谷歌有很多教程，没必要在这里复制粘贴一遍，写这篇文章的主要目的是分享在项目中遇到问题时的一些解决思路，以及这次仓促的完成任务之后的一些感想。 原本以为这次开发完全是Web App，我就是写一下Java代码作为插件让前端去调用，到后面发现打票器的排版功能刚好完全不符合我们的需求，由于时间紧迫，换机器是不可能的了，迫不得已我们决定使用打票机打印图片的功能，我们在图片里面对内容排版好，然后使用打票机进行打印出票。虽然这样会存在一些问题，例如比较小的字会不太清晰，但是这却是看起来最方便也是当前唯一的解决方案了。然而接入扫码器的SDK的时候才意识到，坑总是无处不在的。当然也不能全怪设备供应商，因为他们不清楚我们开发的是Web App，而扫码器的最大问题就在于，供应商提供的SDK并不是跟扫码设备交互的并返回结果的，扫码器本身就自带扫码识别功能，并且在识别之后自动通过USB对系统进行键入，所以SDK它仅仅只充当了一个拦截外部USB键盘设备键入并组装的角色。而Cordova插件这一层是无法对键盘输入进行监听的，所以这个SDK对我们来说完全是没用的。随后我想让前端想办法使用Javascript对键盘输入进行拦截并对所有内容进行组装，然而经过一轮查阅资料和实践后前端同事告诉我在他在安卓浏览器里面无法获取到键入的KeyCode，这。。。 虽然心里的羊驼真的无法言表，但是问题总是要解决的。与其埋怨，还不如动脑筋想一想解决办法。经过大家对扫码器的反复研究，初步制订了两个解决方案。 方案一：通过按钮定位到一个隐藏的input框获得焦点，然后扫描二维码后内容就会被填充到input框内，然后再将获取得到的内容去请求接口得到图片地址再调用打印插件进行打印。 方案二：通过按钮触发跳转到原生安卓界面，在安卓页面内进行键入拦截，然后同样的将二维码内容去请求接口得到图片地址再调用打印插件进行打印，打印完毕后跳转回原WebView页面。 实际上我们在操作方案一的时候遇到了非常多的问题，例如input框获得焦点会使得系统软键盘弹起，又例如键入的时候若软键盘的语言是中文的话，那么键入的内容就会出问题。因此方案一实在是不靠谱的方案，很快就被否决了。 于是我们很快就决定实践方案二了，虽然我大学是学Java入门的，但是对安卓完全不熟悉，现在才找人来接手也不太现实。所以也只能硬着头皮上了，毕竟需要做的只是一个简单的页面，应该没多困难。整个方案二实现过程大都非常顺利，包括原生页面与到WebView页面相互跳转、接入扫码器的SDK、调用出票机的SDK，这几个流程的代码没有出现任何问题，整个过程大家调试起来都比较轻松愉快。然而暴风雨来临之前总是宁静的，不知道大家是否还记得我们扫码获得的数据是需要通过接口再去换取图片地址然后才能进行打印的，其实最大的问题就埋藏在了这里。 我们最后将接口接入，并调用之前给Js调用的插件，然而发现程序并不能正常运作。随后我们将插件部分的代码单独拿出来执行，才找到了问题出在了网请求上面，因为网络请求在主线程上面执行是会抛出异常的。所以在查阅资料之后，我们尝试了另起线程进行网络处理，然而也没有成功。我承认当时是的氛围是急躁的，因为大家已经加班快十个小时了，却卡在最后这一步，而大佬们给的建议是使用网络请求库。我们都清楚使用库是需要不少时间成本去看文档的，所以大家的情绪都比较的消极。但我也没用纠结于刚刚的代码为何不能正常运行，而是决定静下心来好好看一下大家介绍的网络请求库，最终使用了async-http-client，虽然说这个库不建议安卓2.2之后版本使用，但是由于有一份乙方公司的代码也使用了这个库，我们起码可以有个比较靠谱的参考，对比起在搜索引擎的条目里进行筛选，我想这份能在真机上运行的代码还是个比较好的选择。 很快，我们临摹着乙方的代码，写出了我们的自己的网络请求，然后再写上了一堆签名的算法，终于，接口请求成功了，图片的地址我们获取回来了。然而，一波刚平一波又起。是的，我们还需要另外一个网络请求进行图片下载。我们在异步请求成功后再对图片资源进行请求，结果这时候发现无论是进行同步还是异步的网络请求，程序都会闪退。由于经验缺乏，我们到最后都没有找到原因，无奈之下，我们选择了修改接口，将原来返回图片地址改为了返回图片的base64数据。这并不是理想的解决方案，因为这样无疑是给我们的接口服务带宽带增加了不少压力。虽然对于我们来说肯定是心有不甘，但至少下一个工作日我们可以及时向客户展示demo了，之后的优化我们就需要专业的安卓工程师来介入了。 我很庆幸这一次的任务能够按时完成，说实话，在接受这项任务的时候我心里并没有十足的把握，也欠缺了自己的思考，结果在实现的过程中不断的遇到了各种难题，这让我想起了The Clean Code其中一章，关于学会说“不”的重要性。可能大家会觉得说“不”就是承认自己能力不足，说“不”就是不肯为公司为团队付出努力，说“不”就是不专业的表现。但是事实却恰恰相反，说“不”是我最专业的回答，因为我绝不会让公司和客户去承担所谓的“试一试”或者“我尽力”所带来失败的结果。我说“不”是提前让领导和公司清楚在限定的时间和指定的技术范畴内我并不能胜任这一次的任务，当然，领导可能会用各种方式诱导我接受这项任务，但是专业的程序员就要坚守自己的底线。我们都知道任何功能的解决方案都不止一个，同样的，我们的领导也有足够的资源让他们能够找到更加合适更加专业的人员去接手任务。只有准时交付以及高质量的软件才能够使得公司和客户双赢，如果你心里没有底，请务必说“不”。]]></content>
      <categories>
        <category>挖坑</category>
      </categories>
      <tags>
        <tag>Hybird</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一款简洁高效的绘图工具]]></title>
    <url>%2F2019%2F01%2F16%2F%E4%B8%80%E6%AC%BE%E7%AE%80%E6%B4%81%E9%AB%98%E6%95%88%E7%9A%84%E7%BB%98%E5%9B%BE%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[工欲善其事，必先利其器 前言 前一阵子准备动手写一些关于数据结构的文章，却发现仅仅靠文字去描述是不够的，特别是当数据结构十分复杂的时候，即使你能够用文字表达清楚，但是看的人也很难完全的理解。因此我们需要图解来帮助我们理解整个数据结构的构成以及理清各个部分之间的关系。 当然，可能有人会用画图、Word这种常用而且操作简单的工具；也有的人是Adobe大神，用PS、AI之类的工具。虽然这里不讨论孰优孰劣的问题，但个人认为对于开发人员来说，命令行应该是最友好的，而图形界面会很大程度的降低效率，所以下面推荐一款简洁高效的绘图工具。 介绍 Graphviz是AT&amp;T Labs Research开发的一个开源的图形绘制工具，它使用一个特定的DSL(领域特定语言):DOT语言作为脚本语言，然后使用布局引擎来解析此脚本，并完成自动布局。它可以很方便的用来绘制结构化的图形网络，能够在Windows、Unix和Unix-like上运行，也支持多种格式输出。 Graphviz中包含了众多的布局器： dot： 有向图的层次图或分层图。如果边缘具有方向性，则使用此默认工具 neato： 用于sprint model的生成（在Mac OS版本中称为energy minimized） twopi： 用于放射状图形的生成 circo： 用于圆形图形的生成 fdp： 另一个用于生成无向图的工具 sfdp： fdp的多尺度版本，用于大图形的布局 使用 Graphviz包含3种元素，图，顶点和边。每个元素都可以具有各自的属性，用来定义字体，样式，颜色，形状等。它设计初衷是对有向图/无向图等进行自动布局，开发人员使用DOT脚本定义图形元素，然后选择算法进行布局，最终导出结果。所以使用Graphviz的一般流程为： 定义一个图，并向图中添加需要的顶点和边 为顶点和边添加样式 使用布局引擎进行绘制 接下来我就展示如何使用各种布局去构造一些数据结构图解。 digraph G{ a; b; c; d; a -&gt; b -&gt; c; b -&gt; d; } 上面的语句非常的简单，下面就是由此生成的图： 图示非常的简单明了，代码也如此，我想不需要解说什么。接下来我们来进行稍微复杂一点的操作。 digraph G { size =&quot;4,4&quot;; main [shape=box]; /* this is a comment */ main -&gt; parse [weight=8]; parse -&gt; execute; main -&gt; init [style=dotted]; main -&gt; cleanup; execute -&gt; { make_string; printf} init -&gt; make_string; edge [color=red]; // so is this main -&gt; printf [style=bold,label=&quot;100 times&quot;]; make_string [label=&quot;make a\nstring&quot;]; node [shape=box,style=filled,color=&quot;.7 .3 1.0&quot;]; execute -&gt; compare; } 这次我们的代码貌似复杂了不少，先不着急看代码做了什么，先看看得到的图是怎样的： 现在先假设我们完全不懂语法（实际上也是这样😂），我们可以通过调节参数来看一下每个参数是用来做啥的。size参数是用来调节输出的图片大小（英寸）的，如果因为觉得图片节点太多而图片太小看起来很费劲的话，就可以使用该参数将图片放大。而[shape=box]这句则是指定了节点的形状，如果不指定，就会默认椭圆。随后我们调整了[weight=8]参数，发现没有它的话，main -&gt; parse之间的连线会变斜，查看了官方文档后发现，[weight]参数默认是1，如果你需要让它在排版的时候连线正直的话，就需要调整[weight]值。[style=dotted]是将连线改为虚线。我们还发现了一对多关系还可以用{}输出，而不需要写多行关系。使用edge [color=red]能够使得连线颜色改变，但是改变的地方是从你声明了edge之后的部分。还可以使用[style=bold,label=&quot;100 times&quot;]对连线进行加粗，以及添加注解。最后在声明了node [shape=box,style=filled,color=&quot;.7 .3 1.0&quot;]之后的compare节点，我们可以看到它改变了形状以及填充了颜色。 上面我们只对节点和连线进行了样式上的修改，接下来我们再看稍微复杂一点的节点： digraph structs { node [shape=record]; struct1 [shape=record,label=&quot;&lt;f0&gt; left|&lt;f1&gt; mid\ dle|&lt;f2&gt; right&quot;]; struct2 [shape=record,label=&quot;&lt;f0&gt; one|&lt;f1&gt; two&quot;]; struct3 [shape=record,label=&quot;hello\nworld |{ b |{c|&lt;here&gt; d|e}| f}| g | h&quot;]; struct1 -&gt; struct2; struct1 -&gt; struct3; } 这片代码看起来貌似不是很直白，那我们还是先看看结果图是怎样的： 看到了图之后，是不是觉得豁然开朗，其实label就是用来修改整个节点的内容的，使用|进行节点分割，支持\n、\r等制表符，还支持类似html的标签。但是上面的一些标签仅仅用作了标识，而没有实际使用，下面我们再来使用html来让节点变得更加丰富。 digraph html { abc [shape=none, margin=0, label=&lt; &lt;TABLE BORDER=&quot;0&quot; CELLBORDER=&quot;1&quot; CELLSPACING=&quot;0&quot; CELLPADDING=&quot;4&quot;&gt; &lt;TR&gt;&lt;TD ROWSPAN=&quot;3&quot;&gt;&lt;FONT COLOR=&quot;red&quot;&gt;hello&lt;/FONT&gt;&lt;BR/&gt;world&lt;/TD&gt; &lt;TD COLSPAN=&quot;3&quot;&gt;b&lt;/TD&gt; &lt;TD ROWSPAN=&quot;3&quot; BGCOLOR=&quot;lightgrey&quot;&gt;g&lt;/TD&gt; &lt;TD ROWSPAN=&quot;3&quot;&gt;h&lt;/TD&gt; &lt;/TR&gt; &lt;TR&gt;&lt;TD&gt;c&lt;/TD&gt; &lt;TD PORT=&quot;here&quot;&gt;d&lt;/TD&gt; &lt;TD&gt;e&lt;/TD&gt; &lt;/TR&gt; &lt;TR&gt;&lt;TD COLSPAN=&quot;3&quot;&gt;f&lt;/TD&gt; &lt;/TR&gt; &lt;/TABLE&gt;&gt;]; } 这里我们就直接把一个table放到了label里面去了，实际上是不是跟html的table一样呢： 把html片段用浏览器打开对比一下，可以说真的没差了。 既然复杂的节点我们已经会编写了，但是复杂的节点意味着节点之间的关系可以多种多样，那么如何将节点之间的关联关系对应清楚呢？我们不妨先观察下图： 大家可以看到图中每个节点的两翼部分分别指向对应的节点的字母部分，如果按我们之前的知识，我们只能做到字母部分之间相互指向，那么我们就来看看这里的代码有什么不同： digraph g { node [shape = record,height=.1]; node0[label = &quot;&lt;f0&gt; |&lt;f1&gt; G|&lt;f2&gt; &quot;]; node1[label = &quot;&lt;f0&gt; |&lt;f1&gt; E|&lt;f2&gt; &quot;]; node2[label = &quot;&lt;f0&gt; |&lt;f1&gt; B|&lt;f2&gt; &quot;]; node3[label = &quot;&lt;f0&gt; |&lt;f1&gt; F|&lt;f2&gt; &quot;]; node4[label = &quot;&lt;f0&gt; |&lt;f1&gt; R|&lt;f2&gt; &quot;]; node5[label = &quot;&lt;f0&gt; |&lt;f1&gt; H|&lt;f2&gt; &quot;]; node6[label = &quot;&lt;f0&gt; |&lt;f1&gt; Y|&lt;f2&gt; &quot;]; &quot;node0&quot;:f2 -&gt; &quot;node4&quot;:f1; &quot;node0&quot;:f0 -&gt; &quot;node1&quot;:f1; &quot;node1&quot;:f0 -&gt; &quot;node2&quot;:f1; &quot;node1&quot;:f2 -&gt; &quot;node3&quot;:f1; &quot;node4&quot;:f2 -&gt; &quot;node6&quot;:f1; &quot;node4&quot;:f0 -&gt; &quot;node5&quot;:f1; } emmm，看起来也很好理解，使用&quot;节点名&quot;:html标签名这样就能够定位到节点的每个部分了。即使你想要指向的部分在很深入的地方，只要声明对了，就没有它达不到的地方： 下面我们继续讲的是子图的使用，talk is cheap，and i just want to show you the code。下面我们就将图一中的节点a、b、c、d拆分成两部分： digraph G{ subgraph cluster0 { a; b; c; } subgraph cluster1 { d; } a -&gt; b -&gt; c; b -&gt; d; } 这部分代码也很清晰明了，subgraph就是用来声明子图的了，但是需要注意的是，子图的名称必须以cluster开头，否则Graphviz是无法识别的。 如果觉得上面的例子太过简单，那么可以研究一下下面这个文档里面的demo： digraph G { subgraph cluster0 { node [style=filled,color=white]; style=filled; color=lightgrey; a0 -&gt; a1 -&gt; a2 -&gt; a3; label = &quot;process #1&quot;; } subgraph cluster1 { node [style=filled]; b0 -&gt; b1 -&gt; b2 -&gt; b3; label = &quot;process #2&quot;; color=blue } start -&gt; a0; start -&gt; b0; a1 -&gt; b3; b2 -&gt; a3; a3 -&gt; a0; a3 -&gt; end; b3 -&gt; end; start [shape=Mdiamond]; end [shape=Msquare]; } 我们都知道节点能使用style和color来定义颜色背景，而这里cluster0和cluster1也一样使用了style和color来定义了子图的颜色背景，并且使用label来定义了子图内的标签文字，我们由此知道子图实际上也拥有一些元素的基础属性。 以上已经是关于dot布局的全部内容了，可以说DOT语言十分的简单明了，当然，这跟它设计的初衷是相符的，即容易上手，也能非常高效的生成我们需要的图片，可以说是图示神器了。如果各位童鞋有兴趣继续深入学习Graphviz其它布局的使用以及DOT语言，可以直接访问官网：Graphviz - Graph Visualization Software，里面介绍了所有你想要知道关于Graphviz的故事，当然文档也十分的丰富和详尽😄。]]></content>
      <categories>
        <category>软件</category>
      </categories>
      <tags>
        <tag>other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年终总结]]></title>
    <url>%2F2018%2F12%2F30%2F2018%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[时光荏苒，日月如梭，不知不觉我们已经走到了 2018 年的尽头。 工作 2018 年是我职业生涯的第二年，却是在三易的新开始。年初的时候，公司的一切对我来说都是那么陌生，我的目标就是能尽快的跟大家伙混个脸熟。然而事与愿违，由于工位不足，我只能被安排在其他项目组的办公室里面进行工作，虽说平时跟附近的同僚们也会聊上几句，但由于工作内容并无交集，所以并没擦出有多少火花。 幸运的是后来重新编排了座位，沟通终于方便了，再加上大家加了几次班，友谊的小船慢慢在变大。沟通方便了，那很多事情做起来都顺畅了很多，例如对产品需求的疑问，对代码的的疑问，都能够很快的得到解答。似乎所有事情看起来都那么井井有条，公司的业务也在不断发展，项目组的人数也在噌噌的往上涨。 在 2018 年的工作中，我们最主要的任务就是完成新需求的开发以及修复 BUG ，而让我印象深刻的任务或者紧急事件主要有以下这些： 抽奖活动事故 微信推文定时推送 特殊的 UV 统计 之所以印象深刻必然是因为它们有特殊之处。就例如事故1发生的那天，负责该项目的同事因为连夜赶工调了休，然后由我来负责追踪问题。获取项目权限，理清项目结构，再了解完大概流程才真正开始寻找问题，花费了很多不必要是时间。这让我意识到了即使项目再小，也要提前分配后勤人员，才能以防万一。 而任务2、3的特殊性在于我考虑这些任务的解决方案的时候，能够跳出自己旧的知识体系，去了解更多的方法和数据结构，以及如何灵活的解决问题。我清楚的知道即使我用已掌握的知识也能够实现这些功能，但是我更知道能够实现和优雅的实现并不是一回事，它们所考虑的东西不止差一丁半点。 无论是任务还是事故，明面上是考验解决问题的能力，但是暗地里校验着人的决策能力。解决问题的方案有很多种，有的人会去找轮子，而有的人会改轮子，还有的人会写轮子，没有孰优孰劣的说法，只有合适与否。合适与否又要考虑到很多因素，时效、成本、拓展、性能、安全等，所以你最终选择的方案代表了你所有的思考。 计划 回首 2018 ，我在年初给自己定下了几个计划： 努力学习，认真工作，学以致用（1/1） 保持日常记录笔记和温习的习惯（1/1） 希望输入之余也能够对外输出知识（1/1） 复习高等数学、线性代数等基础知识（1/2） 重新啃一遍操作系统，争取看懂50%并实践（0/1） 假如学有余力，希望可以摸一下AI技术的大门（0/1） 前些年买了很多书没看，争取今年读透2本技术相关书籍（0.5/2） 计划看上去十分的饱满而充实，但是有句古话说得好：计划赶不上变化，所以上面一些的比较细粒度的计划并没有执行得很好😂。如大家所见每条计划后面的比例就是完成度，实际上因为觉得第4点和第6点在短期内对个人的提升来说不会有太大作用，所以中途决定暂停了这两项计划。但是我认为也并没有浪费了4、5、6、7点计划的时间在毫无用处的地方上面，而是将它们用在了其他的一些的计划上面： 保持每天一定的运动量（207/273） 除了身体的运动脑子也要运动，数学题或算法题都能帮到我（1/1） 加入学习英语打卡队伍，扩充词汇量，希望日后能够比较流畅的交流（120/121） 这部分的计划几乎都是年中才决定的，决定重拾运动的计划是因为易胖和食神体质的我自从去年停止了运动之后已经重了二十多斤了，最重要的是再胖下去会影响到身体健康。而第二点是因为在看技术文章的时候，发现有时候会卡在解决思路和数据结构里面不能理解，所以我认为最好的方法就是多看多练多动脑。而第三点则是老同学想找人一起承担一些课程套餐的 quota ，因为一个人很难在限定时间内用完所有的 quota ，我考虑了一下就加入到队伍里面去了。 目标 上面主要陈述的是今年的一些计划，但凡说到计划，就不能没有目标，没有计划的目标是空想，没有目标的计划是瞎做。虽说最终的目标肯定是自身的成长和价值的体现，但是实际上一年时间说多还真不多，想要说实现了这么泛而宏大的目标是不切实际的，所以我在这里归纳总结，希望能描绘出通往最终目标路上的已完成的一些小目标： 乐观、积极向上的生活状态，享受生活，爱我所爱 学习不停留在应用层面，希望能深入弄懂内在的原理和实现方式 造一些轮子，尽管可能不够别人的好，但你的轮子也能有自己的特色 在肥胖的道路上停下了脚步，希望能重新踏上健康的大道 英语词汇量有一定的提升，沟通交流上有较为明显的进步 以上五点皆为我认为在 2018 年达到了的一些小目标，大部分是学习相关的目标，而只有第一点是关于生活的。说实话关于 2018 生活方面的总结好像是挤不出别的东西了，因为相比去年更多的时间都是宅在家里，跟同学朋友联络也不算少，但绝大多数都是在开黑语音上面😂。 未来 以上就是我对于 2018 年的总结，有进步，也有不足。而对于 2019 ，我有以下几点展望： 持续学习，不要给自己设定边界，学无止境 加强外交，不能仅仅在网络交流，要来就来真的 阅览群书，不止于技术相关书籍，更多的了解技术之外的世界 关于 2018 年，丧的事情有千千万，我都不在文中提及。 Life is like a roller coaster，总是起起落落落落落落。但，塞翁失马焉知非福。我们只管风雨兼程，时间会给出最后的答案。 完。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>summary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis之神奇的HyperLogLog]]></title>
    <url>%2F2018%2F11%2F18%2FRedis%E4%B9%8B%E7%A5%9E%E5%A5%87%E7%9A%84HyperLogLog%2F</url>
    <content type="text"><![CDATA[这篇笔记想码很久了，于是今天码完上一篇二维码的文章后立马先占个坑，免得后面又被其他题材冲掉了。 在某一天开完了需求会议之后，我新的工作任务中包含了一项统计浏览量的任务。关于这项任务的详细需求大致如下： 每个用户（user_id）每天浏览过该数据(1~n次)算一个单位浏览量 能够统计每条数据一直以来总的浏览量 能够得出15天内的按浏览量排序总排行榜和地区排行榜 能够得出15天内的实际的UV总排行榜和地区排行榜 允许存在数分钟的数据更新延迟 允许出现小程度误差 由于怕概念有点混淆，所以我这篇文章里将以一天为单位的UV称为浏览量，将以15天为单位的UV称为UV 相信大家看到上面列出的要点后也进行了一轮思考，不知道大家是否理解了浏览量和UV的概念。或许大家能够想到很多种方案，不管是轻捷的、复杂的、常规的还是骚气的，都希望大家可以分享一下，而我在此也分享一下自己的思路以及实现过程。 从标题中就可以看到，本文将使用到Redis，以及Redis新支持的一种数据类型HyperLogLog。实际上一开始我并不清楚HyperLogLog的使用方法，但是从需求出发，有序集合SortedSet是需要的，而剩下的步骤，就看自己对Redis的理解能不能骚出点东西来了。 由于Redis的特性，我认为它能够成为一个很好的中转站，但不应该是一个仓库，所以总的浏览量我最终会选择存储到Mysql等关系型数据库中，而Redis缓存下来的，则是15天内的数据。假设我们的SortedSet中存储的是汇总了15天内的浏览量数据，那么第一个问题，就是我们如何计算得出总浏览量。如我们定时将缓存数据存入Mysql，而Mysql中我们只用一个字段存储这个总浏览量，那么那么我们在第16天的时候，总浏览量就会丢失掉第一天的数据。所以我在这里使用了三个字段来协助总浏览量的存储，一个是当天浏览量，第二个是今天前的总浏览量，剩下一个是缓存更新时间，为了方便，下面我们会将： 总浏览量称为total_view 当天浏览量称为day_view 今天前的总浏览量称为total_view_bt 缓存更新时间称为cache_update_time 所以最终我们的total_view = day_view + total_view_bt，当定时更新数据库数据的时候，如果当前时间跟cache_update_time是同一天，则只更新day_view值，如果不是同一天，则会先将total_view_bt更新为total_view_bt + day_view，然后再将day_view替换为Redis中的当天浏览量数据。 这样我们Mysql的总浏览量存储是搞定了，那么接下来我们就需要用Redis存储好这15天的浏览量了，那这每天的数据我们应该用什么结构去存储呢？我们先来回顾一下需求，首先我们的需求只需要统计浏览量，而不需要知道这个数据具体被哪些用户浏览过，那么就是说明我们不需要知道我存储的实际内容，而另一方面，我们需要的是最近15天的数据，而15天前的数据已被存储到Mysql中，所以第1天的数据应该在第16天凌晨就过期了。 终上所诉，在大家都熟悉的Redis五种数据类型中，能满足需求的，我个人认为只有SortedSet是比较符合的（注意这里的SortedSet跟上面汇总的SortedSet不是同一个）。如果使用SortedSet，我想到两种方案，一种是以data_id + user_id + day作为SortedSet里面元素的键，以过期时间作为score通过定时任务剔除过期数据；另外一种是使用data_id + day作为键，即一个数据存储15个SortedSet，然后再单独设置过期时间。但是上述两种方案都存在一些比较麻烦的问题，例如： 方案一需要代码维护SortedSet；方案二需要复杂的去重统计UV 方案一和方案二都需要一次性获取全部数据进行统计浏览量以及排序 SortedSet的插入需要O(N)的时间复杂度，影响效率 我们完全不需要知道我存储的实际内容，使用SortedSet比较浪费空间 认真的思考过后发现存在较多的问题，那到底还有没有其他比较好的方案可以使用呢？或许我们可以开阔一下视野，不要局限于我们常用的数据类型上。后来我是想起了之前比较热门的一篇文章：如何判断一个数是否在40亿个整数中？，这篇文章讲到了bitmap的使用，而bitmap也是Redis最新支持的数据类型之一，于是我就去阅读关于Redis新支持的两种数据类型的文章了。看完之后豁然开朗，发现bitmap和HyperLogLog都比较满足我们对效率、空间和简单编码的要求，最后就采用了HyperLogLog的数据类型去实现我们的需求。 先简单的介绍一下HyperLogLog的特性和操作，首先HyperLogLog是用来做基数统计的，基数统计指的是指统计一个集合中不同的元素的个数，下面摘录一段源于《HyperLogLog the analysis of a near-optimal cardinality estimation algorithm》的简述： 在理想状态下，将一堆数据hash至[0,1]，每两点距离相等，1/间距 即可得出这堆数据的基数。然而实际情况往往不能如愿，只能通过一些修正不断的逼近这个实际的基数。实际采用的方式一是分桶，二是取kmax。分桶将数据分为m组，每组取第k个位置的值，所有组中得到最大的kmax，(k-1)/kmax得到估计的基数。 HyperLogLog算法的另一个主观上的理解可以用抛硬币的方式来理解。以当硬币抛出反面为一次过程，当你抛n次硬币全为正面的概率为1/2n。当你经历过k(k很大时)次这样的过程，硬币不出现反面的概率基本为0。假设反面为1，正面为0，每抛一次记录1或者0，当记录上显示为0000000…001时，这种可以归结为小概率事件，基本不会发生。转换到基数的想法就是，可以通过第一个1出现前0的个数n来统计基数，基数大致为2(n+1)时。硬币当中可以统计为(1/21+1/42+1/8*3…)，大致可以这么去想。 有兴趣深入了解的童鞋可以Google一下上面的论文，下面我们回归到如何在Redis中操作HyperLogLog上来。Redis中一共有三种操作HyperLogLog的方法： PFADD: 将任意数量的元素添加到指定的HyperLogLog里面。作为这个命令的副作用，HyperLogLog内部可能会被更新， 以便反映一个不同的唯一元素估计数量（也即是集合的基数）。如果HyperLogLog估计的近似基数在命令执行之后出现了变化， 那么命令返回1， 否则返回0。 如果命令执行时给定的键不存在， 那么程序将先创建一个空的HyperLogLog结构， 然后再执行命令。 PFCOUNT：当PFCOUNT命令作用于单个键时， 返回储存在给定键的HyperLogLog的近似基数，如果键不存在，那么返回0。当PFCOUNT命令作用于多个键时，返回所有给定HyperLogLog的并集的近似基数，这个近似基数是通过将所有给定HyperLogLog合并至一个临时HyperLogLog来计算得出的。通过HyperLogLog数据结构，用户可以使用少量固定大小的内存，来储存集合中的唯一元素（每个HyperLogLog只需使用12k字节内存，以及几个字节的内存来储存键本身）。命令返回的可见集合（observed set）基数并不是精确值，而是一个带有0.81%标准错误（standard error）的近似值。 PFMERGE：将多个HyperLogLog合并（merge）为一个HyperLogLog，合并后的HyperLogLog的基数接近于所有输入HyperLogLog的可见集合（observed set）的并集。合并得出的HyperLogLog会被储存在destkey键里面，如果该键并不存在，那么命令在执行之前，会先为该键创建一个空的HyperLogLog。 知道了如何操作HyperLogLog后我们就开始用它来实现我们的需求吧。 首先我使用data_id + day作为SortedSet的键记录该资源数据当天的UV，其中day是由当前时间戳除以一天的秒数然后再模15得出的一个0~14的数，即day = (1543127511 // 86400) % 15，然后用户浏览的时候将用户的user_id存进去以及设置好过期时间，最后将data_id_day1到data_id_day15的数据循环用PFCOUNT取出来后相加的总和是每天的UV总和（即理解为15天内的浏览量），简称之为15UDV，而将data_id_day1到data_id_day15的数据一次性使用PFCOUNT得出来的结果是15天的UV结果，简称UV。那么15UDV和UV是两种不同排序依据，所以我们这里将此资源数据的UV值存入到最后用于排序的SortedSet里面。由于有总排行榜和城市排行榜，所以我们需要定义两种SortedSet： rank：15天UV总排行榜 rank_[city]：city替换为对应资源所在城市的15天UV城市排行榜 至于15UDV嘛，其实跟上面UV也是一样的操作，就不多说了。所以到这里所有步骤已经完成了，下面就展示一下的伪代码方便大家再理顺一下： # 存储过程 data_id = 1 city = 'guangzhou' user_id = 'ove1d291d2912ed1ad91e2' day = (now.timestamp // 86400) % 15 15day_after = now.date.timestamp + 15*86400 key = str(data_id) + '_' + str(day) this.redis.multi() this.redis.pfadd(key, user_id) this.redis.expireat(key, 15day_after) this.redis.exec() # 获取UV for i in range(15): keys = [str(data_id) + '_' + str(i)] UV = this.redis.pfcount(keys) # 获取15UDV 15UDV = 0 for i in range(15): udv_key = str(data_id) + '_' + str(i) 15UDV += this.redis.pfcount(udv_key) # 存储到排行榜中 rank_key = 'rank' city_rank_key = rank_key + '_' + city udv_rank_key = 'udv_rank' udv_city_rank_key = udv_rank_key + '_' + city this.redis.multi() this.redis.zadd(rank_key, UV, data_id) this.redis.zadd(city_rank_key, UV, data_id) this.redis.zadd(udv_rank_key, 15UDV, data_id) this.redis.zadd(udv_city_rank_key, 15UDV, data_id) this.redis.exec() # Mysql统计总浏览量 resource = orm.table('resource').get(data_id) if date(resource.cache_update_time) != date(now.timestamp): resource.total_view_bt += resource.day_view resource.day_view = this.redis.pfcount(key) resource.cache_update_time = now.timestamp resource.total_view = resource.total_view_bt + resource.day_view resource.save() 完。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二维码生成原理及编码实现]]></title>
    <url>%2F2018%2F10%2F16%2F%E4%BA%8C%E7%BB%B4%E7%A0%81%E7%94%9F%E6%88%90%E5%8E%9F%E7%90%86%E5%8F%8A%E7%BC%96%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一年前挖的坑，现在来补，希望不会太晚。先把压舱底的笔记放上来以表我会填完坑的决心。当前还在整理和重写旧代码阶段，下面先把一些基础知识罗列出来。 11月9日更：最近工作比较忙，而且二维码的规则真的很烦杂多，虽然看上去总体代码量不多，结构也很随性，但是也花了半个月时间才完成了初版的功能。按之前说的，下面我会把一些基础知识列出来，但不会太过详尽，到编码实战章节将会详细解说。在仓库中有一份专门讲解二维码的构成以及解析规则的PDF全英说明书，如果大家觉得有哪点不太清楚的可以去PDF里面找-----&gt;代码仓库地址 简要笔记 40个标准版本、4个微型版本 四种编码方式： 数字：0-9 大写字母和数字：0-9，A－Z，空格，$%*±./: 二进制/字节：ISO/IEC 8859-1 日本汉字/假名：shift JISJIS X 0208 四种容错级别 L：7%字码可被修正 M：15%字码可被修正 Q：25%字码可被修正 H：30%字码可被修正 结构 版本信息 格式信息 数据及容错密钥 数据需求模块 定位标识 校正标识 定时标识 静态区域 功能性图样： 闷声区 定位标识 分隔符 定时标识 校正标识 编码区域： 格式信息 版本信息 数据及容错字码 编码QRCode流程： 数据分析 编码数据 计算容错码 组织数据 填充 应用数据掩码 填充格式信息和版本信息 基础图解 二维码的英文是QR Code(Quick Response Code)，它本质上是一种密码算法，近几年来成为了移动设备上非常流行的一种编码方式，它比传统的条形码能存更多的信息，也能表示更多的数据类型。二维码有 40 种尺寸，我们称其为Version。Version 1是21×21的矩阵，Version 2是25×25的矩阵，可以看到，每增加一个 Version，尺寸都会增加 4，所以尺寸Size与Version的关系为：Size = (Version - 1)*4 + 21。而Version的最大值是 40，故尺寸最大值是(40-1)*4+21 = 177，即 177 x 177 的矩阵。 结构示意图 二维码的组成基本上可被分为定位、功能数据、数据内容三部分。 定位图案 定位标识：用于标记二维码矩形的大小；用三个定位图案即可标识并确定一个二维码矩形的位置和方向了； 分隔符：用白边框将定位图案与其他区域区分； 定时标识：用于定位，二维码如果尺寸过大，扫描时容易畸变，时序图案的作用就是防止扫描时畸变的产生； 校正标识：只有在 Version 2 及其以上才会需要； 功能数据： 格式信息：存在于所有尺寸中，存放格式化数据； 版本信息：用于 Version 7 以上，需要预留两块 3×6 的区域存放部分版本信息； 数据内容： 数据 容错字码 纠错码、掩码、纠错级别信息记录标识 数据编码 接下来介绍二维码的编码方式。 Numeric mode 数字编码 从0到9。如果需要编码的数字的个数不是3的倍数，那么，最后剩下的1或2位数会被转成4或7bits，则其它的每3位数字会被编成 10，12，14bits，编成多长还要看二维码的尺寸 Alphanumeric mode 字符编码 包括 0-9，大写的A到Z（没有小写），以及符号$ % * + – . / : 包括空格。这些字符会映射成一个字符索引表。如下所示：（其中的SP是空格，Char是字符，Value是其索引值） 编码的过程是把字符两两分组，然后转成下表的45进制，然后转成11bits的二进制，如果最后有一个落单的，那就转成6bits的二进制。而编码模式和字符的个数需要根据不同的Version尺寸编成9, 11或13个二进制 Byte mode 字节编码 可以是0-255的ISO-8859-1字符。有些二维码的扫描器可以自动检测是否是UTF-8的编码。 Kanji mode 日文编码 也是双字节编码。同样，也可以用于中文编码。日文和汉字的编码会减去一个值。如：在0X8140 to 0X9FFC中的字符会减去8140，在0XE040到0XEBBF中的字符要减去0XC140，然后把结果前两个16进制位拿出来乘以0XC0，然后再加上后两个16进制位，最后转成13bit的编码。 其他编码 Extended Channel Interpretation (ECI) mode 特殊字符集 主要用于特殊的字符集，并不是所有的扫描器都支持这种编码 Structured Append mode 混合编码 说明该二维码中包含了多种编码格式 FNC1 mode 特殊行业编码 主要是给一些特殊的工业或行业用的，如GS1条形码等 掩码、纠错码等 在数据编码完成后，为了防止出现大面积的空白或黑块而导致识别困难，我们还需要做多一步操作。一共有8种掩码你可以使用，下图中包含八种掩码的图示以及公式。所谓掩码，就是和上面生成的图做异或操作，而且掩码只会作用于数据区域。最最后，我们得到的八种掩码与数据其余异或出来的数据矩阵实际上都可以作为二维码被识别，但是考虑到要让扫码器更加容易的识别，需要从八个数据矩阵中筛选出机器最容易辨别的，筛选的依据又涉及到一套惩罚计算的规则了。 掩码类型 二维码的纠错码主要是通过Reed-Solomon error correction来实现的。对于这个算法，可以说是相当的复杂，在短时间内也许我们不能完全理解他的算法含义，但是可以先了解它的一些基础知识以及步骤，如果觉得必须要完完全全搞清楚，建议查看相关的论文。由于本篇文章篇幅有限，在未完全理解该算法的情况下我不可能妄自总结，所以这里先推荐一篇稍微简单的介绍文章给各位感兴趣的读者：为程序员写的Reed-Solomon码解释 编码实战 骨架 好了本节进入了编码实战阶段，我们首先需要做的是先将整个二维码生成流程理清楚，并构造好对应的函数。 #!/usr/bin/python class Qrcode: mode = None level = None qrcode = None version = 1 data_matrix = None mask_id = None length = 0 size = () def generate(self, path): return def resize(self, size): return def paint(self, img, fg_or_bg=0): return def _matrix_to_img(self, img_mode='1', matrix=None): return def __init__(self, message, level_index='L'): return def decide_version(_message, _level_index): return def build_matrix(encode_data): def build_locate_sign(): return def build_time_sign(): return def build_dark_sign(): return def build_alignment_sign(): return def level_and_mask_build(_mask_id): return def build_version_info(): return def data_build(_encode_data): return def mask(): def mask_template(col, row, _mask_id): return def penalty(__matrix): return return def encode(_message): def get_data_codewords(__message): return def rs_encode(_data_codewords): return 我的代码抽丝剥茧后剩下的骨架大概就是以上的代码，和一开始我所打造的结构有一点区别，主要还是流程没有完全梳理好所导致的。所以大家常说理解好业务流程是项目环节中很重要的一环，它影响到整个项目的可扩展性以及日后的可维护性。在我个人看来优秀的程序员并不是指代码写得多么好、结构多么清晰、使用的技术多么高大上的人，而是能够真正的理解用户的真实诉求并加以分析以及能够梳理和完善流程的人。这种人无论在哪个行业，做什么样的工作，称之为优秀都是当之无愧的。 版本选择 二维码的版本最直接影响的就是二维码内容容量的多少，当然除了版本外影响容量的还有纠错等级，所以我们最先需要计算的是根据用户的传入信息量和选择的纠错级别得出应该使用哪个版本的二维码。 def __init__(self, message, level_index='L'): self.level = level_map[level_index] message = message.encode() def decide_version(_message, _level_index): if all(chr(i) in num_list for i in _message): mode = 'numeric' else: mode = 'byte' for each_version in range(40): if character_amount[_level_index][each_version][mode_map[mode]] &gt; len(_message): self.version = each_version + 1 if each_version + 1 &gt; self.version else self.version break self.length = 21 + 4 * (self.version - 1) self.size = (self.length, self.length) self.mode = mode decide_version(message, level_index) 为了容易理解，我把上下文的代码也贴到上面了。我使用的是python3，首先我们要先把信息内容encode()转化为一个个的byte，如果你不这样做python3拿出来的数据会是多个byte的组合，因为它已经能够识别多国语言了。接下来就是用每个版本对应纠错等级下的数据容量和当前需要编码的数据量进行对比了，一直到能容纳的数据量比需求的多，就可以停止对比更高的版本了。另外在这里我还进行了编码方式的指定，关于编码方式的说明可以接着看下一章节。 数据编码 数据编码这一节可以说是整个流程中最重要的、最核心的，二维码的数据识别以及纠错能力都是通过这一部分来实现的。大家可能比较奇怪为什么这么重要的一环代码量却并不多，实际上我们项目中并没有实现Reed-Solomon算法，而是使用了依赖库，所以该部分的代码仅仅只是进行数据切割组装以及使用依赖库中封装好的函数。当然，我也愿意为有兴趣查看依赖库源码的童鞋奉上地址：reedsolomon def encode(_message): def get_data_codewords(__message): def numeric_encode(___message): diff_encode_code = '' divided_arr = [___message[i:i + 3] for i in range(0, len(___message), 3)] for _equal_or_less_than_three_digits in divided_arr: respectively_len = 10 - 3 * (3 - len(_equal_or_less_than_three_digits)) diff_encode_code += bin(int(_equal_or_less_than_three_digits))[2:].zfill(respectively_len) return diff_encode_code def byte_encode(___message): diff_encode_code = '' for b in ___message: diff_encode_code += bin(b)[2:].zfill(8) return diff_encode_code mode_encode = { 'numeric': numeric_encode, 'byte': byte_encode, } incomplete_codewords = mode_indicator_map[self.mode] + bin(len(__message))[2:].zfill( character_count_indicator_map[self.version][mode_map[self.mode]]) + mode_encode[self.mode](_message) distance_to_8_multiple = 8 - (len(incomplete_codewords) % 8) incomplete_codewords += '0' * distance_to_8_multiple codewords = incomplete_codewords bytes_need = 8 * each_version_required_bytes[self.version][self.level] while len(codewords) &lt; bytes_need: codewords += '1110110000010001' if bytes_need - len(codewords) &gt;= 16 else '11101100' _data_codewords = [int(codewords[i:i + 8], 2) for i in range(len(codewords)) if not i % 8] return _data_codewords def rs_encode(_data_codewords): _encode_data, data_block, i = '', [], 0 block_codecount = num_of_error_correction_blocks_2_error_correction_per_blocks[self.version][self.level] for group1 in range(block_codecount[0]): data_block.append(_data_codewords[i:i + block_codecount[1]]) i += block_codecount[1] for group2 in range(block_codecount[2]): data_block.append(_data_codewords[i:i + block_codecount[3]]) i += block_codecount[3] nsym = ecc_num_version_level_map[self.version][self.level] gen = rs_generator_poly(nsym) ecc_num = len(gen) - 1 _ecc_data = [] for block in data_block: _data_block_get_ecc_block = block + [0] * ecc_num for i in range(len(block)): coef = _data_block_get_ecc_block[i] if coef != 0: for j in range(ecc_num + 1): _data_block_get_ecc_block[i + j] ^= gf_mul(gen[j], coef) _ecc_data.append(_data_block_get_ecc_block[len(block):]) _all_block_data = ''.join(bin(dec)[2:].zfill(8) for block in zip(*data_block) for dec in block) # 突出部分补全 for block in data_block: if len(block) == block_codecount[3]: _all_block_data += bin(block[block_codecount[3] - 1])[2:].zfill(8) _all_ecc_data = ''.join(bin(dec)[2:].zfill(8) for block in zip(*_ecc_data) for dec in block) return _all_block_data + _all_ecc_data + '0' * remainder_bits[self.version] data_codewords = get_data_codewords(_message) return rs_encode(data_codewords) 这一部分代码大家可以看到最主要由get_data_codewords和rs_encode组成，而get_data_codewords中又包含了numeric_encode和byte_encode这两种编码方式。我想大家应该记得，它们就是数据编码章节中介绍的其中两种编码方式。之所以没有编写其他的编码方式，主要原因有三： 不一样的编码方式实际上只是根据不同的编码所制定的规则不一样而已，万能的规则肯定是最受欢迎的 当今市面上大多数主流的扫码器支持自动检查byte_encode的数据是否是UTF-8的编码，换言之就是基本支持所有的语言的识别 其他的一些编码虽说在某种程度上能够节省一定的空间，可能能够降低二维码使用的版本使得扫码器更容易识别，但是本身过多的信息不应该通过二维码进行传播，而且如今的影像技术在识别高版本二维码上不会有太大的压力 终上所诉，在我的代码中只保留了两种编码方式，一种是对纯数字的编码方式，另外一种则是现在可以对所有编码都适用的编码方式。 说完了关于代码中的编码方式，接下来我们继续解说这段get_data_codewords的代码还干了什么。大家可以看到被编码的数据是最后才加上的，而编码数据前面有： mode_indicator_map[self.mode] + bin(len(__message))[2:].zfill(character_count_indicator_map[self.version][mode_map[self.mode]]) 其中的mode_indicator_map是你选择的编码模式的标识，如果是numeric模式需要加上0001， alphanumeric模式为0010， byte模式为0100， kanji模式为1000。而后面那段看起来很复杂的一串东西，实际上是数据长度的标识，先将输入的文本长度转化为二进制，然后查出不同版本下不同编码模式所需要的（文本长度）标识长度，接着用0去对该二进制数进行补齐缺省长度。最后加上数据编码后，如果整体数据的长度不是8的整数倍，还需要在数据后面用0进行补齐。 也许你已经被上面绕来绕去弄得晕头转向了，但这仅仅只是开始:)。接着，我们的得到的二进制数据长度还不够，我们要查到当前二维码版本以及纠错级别下的二维码需要数据区域的长度为多少，如果还没有达到需要的最大长度，我们还要轮流加两个补齐码11101100和00010001，没错轮流的意思就是11101100000100011110110000010001……。这就是完整的编码区数据编码了，我们称之为Data Codewords，每一个8bits叫一个codeword，我们还要对这些数据码加上纠错信息，所以我们对数据进行每8bits拆分并转化为了十进制，得到了一个充满十进制数的数组。 接下来就是rs_encode()函数的工作了，num_of_error_correction_blocks_2_error_correction_per_blocks这里我有一个常量命名非常的长，它代表着每个版本和纠错等级下所需纠错码的block数量，映射关系的格式为(1, 19, 0, 0)或(2, 38, 2, 39)，如果第三个数为0，说明所有的block所需要的codeword数量是一样的，反则反之，最多可能会有两组block。第二和第四个参数则代表每个block所需要的codeword数量。分组分block后就是到了计算纠错码的步骤了，如果想要理解这部分的详细步骤和原理可以参考 掩码、纠错码等 中我提供的链接。 现在我们手上既有Data Codewords，也得到了对应的纠错码，最后一步就是将他们组合成我们二维码数据区域中所需要的完整的数据了。至于怎么合起来呢，还是需要点骚操作的。上面我们得到的多组多block的Data Codewords和纠错码都是二维数组，那么也就是矩阵啦，接下来我们需要做的是将矩阵转置，然后将里面元素转化为二进制并用0补齐到8位后一行一行的串连起来，Data Codewords和纠错码两个矩阵都需要进行此操作。在这里的处理中我一开始忘了分组后的codeword是不一样的，矩阵转置后会丢掉某一组多出的元素，所以我们需要补上这一部分的数据。 到这里数据编码部分是真正的结束了，是不是有点复杂？没事多看几次就能够熟悉了:) 填充数据矩阵 现在我们离二维码出生只有两步之遥了，接下来都是很简单的东西了:) 基础图解 中可以看到，版本信息、格式信息、定位标志、校正标志、定时标志这些都是不依赖数据内容的（当然，它们受数据长度影响），接下来我们就构造一个二维码的矩阵，先把这些相对固定的数据填充进去。 def build_locate_sign(): for i in range(8): for j in range(8): if i in (0, 6): self.data_matrix[i][j] = self.data_matrix[-i - 1][j] = self.data_matrix[i][ -j - 1] = 0 if j == 7 else 1 elif i in (1, 5): self.data_matrix[i][j] = self.data_matrix[-i - 1][j] = self.data_matrix[i][ -j - 1] = 1 if j in (0, 6) else 0 elif i == 7: self.data_matrix[i][j] = self.data_matrix[-i - 1][j] = self.data_matrix[i][-j - 1] = 0 else: self.data_matrix[i][j] = self.data_matrix[-i - 1][j] = self.data_matrix[i][ -j - 1] = 0 if j in (1, 5, 7) else 1 def build_time_sign(): for i in range(self.length): self.data_matrix[i][6] = self.data_matrix[6][i] = 1 if i % 2 == 0 else 0 def build_dark_sign(): for j in range(8): self.data_matrix[8][j] = self.data_matrix[8][-j - 1] = self.data_matrix[j][8] = \ self.data_matrix[-j - 1][8] = 0 self.data_matrix[8][8] = 0 self.data_matrix[8][6] = self.data_matrix[6][8] = self.data_matrix[8][-8] = 1 if self.version &gt; 6: for i in range(6): for j in (-9, -10, -11): self.data_matrix[i][j] = self.data_matrix[j][i] = 0 def build_alignment_sign(): point_matrix = [] if alignment_location[self.version]: for i in alignment_location[self.version]: for j in alignment_location[self.version]: point_matrix.append((j, i)) matrix_len = len(point_matrix) for index in range(len(point_matrix)): if index == 0 or index == sqrt(matrix_len) - 1 or index == matrix_len - (sqrt(matrix_len) - 1) - 1: continue else: for x_offset in range(-2, 3): for y_offset in range(-2, 3): self.data_matrix[point_matrix[index][0] + x_offset][point_matrix[index][1] + y_offset] \ = 1 if x_offset % 2 == 0 and y_offset % 2 == 0 or abs(x_offset) + abs( y_offset) == 3 else 0 def level_and_mask_build(_mask_id): for format_i in range(len(format_info_str[self.level][_mask_id])): self.data_matrix[format_i if format_i &lt; 6 else ( format_i + 1 if format_i &lt; 8 else self.length - 7 + (format_i - 8))][8] = int( format_info_str[self.level][_mask_id][format_i]) self.data_matrix[8][format_i if format_i &lt; 6 else ( format_i + 1 if format_i &lt; 8 else self.length - 7 + (format_i - 8))] = int( format_info_str[self.level][_mask_id][14 - format_i]) self.data_matrix[self.length - 8][8] = int(format_info_str[self.level][_mask_id][7]) def build_version_info(): if self.version &gt; 6: _version_info = version_info_str[self.version][::-1] for num_i in range(len(_version_info)): self.data_matrix[num_i // 3][num_i % 3 + self.length - 11] = int(_version_info[num_i]) self.data_matrix[num_i % 3 + self.length - 11][num_i // 3] = int(_version_info[num_i]) 上面的代码分别就是定位标志、定时标志、黑点、校正标志、格式信息、版本信息的填充代码了。咦不对，怎么多了个黑点这个奇怪的东西？哈哈，这个东西其实是固定在这里的： 神奇的黑点 定位标志、黑点、定时标志 定位标志大小是不会变的，只有位置会根据版本变化；黑点我想就不用多说了，就是那里的一点；而定时标志就是黑白相间连接在定位标志之间。 定位标识图示 校正标志 从Version 2才开始有校正标志的出现，它们应该出现的坐标记录在常量alignment_location中，例如Version 2中的(6, 18)，代表x轴和y轴分别在6和18上面相交的四个点：(6,6),(6,18),(18,6),(18,18)，其他版本的也是以此类推，下图是Version 8的图示，如果交点上已经被定位标志所占，则该点就不会有校正标志。 而校正标志的尺寸大小就如下图所示： 格式信息 接着就到格式信息出场了，格式信息一共有两条，避免其中一条被遮挡而导致读取不到信息： 格式信息的长度为15bits，每一个bit的位置如下图所示： 这15个bits中，前2个bits用于表示纠错级别，接着3个bits表示掩码，最后10个bits用于纠错的，也是通过reedsolo来计算得出。最后15个bits还要与101010000010010做异或操作。这样就保证不会因为我们选用了00的纠错级别和000的掩码而造成整个格式信息为白色。综上所述，格式信息总共只有(纠错级别*掩码样式)4*8=32种可能，所以我们这里只要把他们最后的结果都列出来，我们就不用做什么计算了，根据纠错级别和掩码选就可以了:) 版本信息 版本信息是从Version 7开始才需要添加的，它会出现在以下两个位置： 版本信息的长度为18bits，其中6bits为版本号，剩下的12bits为纠错码。因为这个也是固定的，所以我们还是老样子，将所有版本的版本信息数据放到了version_info_str里面，按需获取即可。而填充的顺序如下图所示： 编码数据填充 最后就是填充我们的得到的数据区域编码了，填充方式如下图：从右下角开始沿着红线填写我们的各个bits。如果遇到了上面的非数据区域，则跳过。 这一步相对来说比较简单，代码如下： def data_build(_encode_data): up = True bit = (int(i) for i in _encode_data) for _block_end_x in range(self.length - 1, 0, -2): _block_end_x = _block_end_x if _block_end_x &gt; 6 else _block_end_x - 1 for y in range(self.length - 1, -1, -1) if up else range(self.length): for x in (_block_end_x, _block_end_x - 1): if self.data_matrix[x][y] is None: self.data_matrix[x][y] = next(bit, 0) up = not up 应用掩码并选择最佳的二维码 记得之前说过还剩两步，但我掐指一算发现我的心算出现了错误，但这次真的是到了最后两步了:) 如果童鞋们忘记了掩码是干嘛用的，可以回到 掩码、纠错码等 回顾一下。那么一共有八种掩码，他们的代号和表达式的映射关系如下： 那我们首先要做的就是把表达式写好了。 def mask_template(col, row, _mask_id): if _mask_id == 0: return (col + row) % 2 == 0 elif _mask_id == 1: return row % 2 == 0 elif _mask_id == 2: return col % 3 == 0 elif _mask_id == 3: return (row + col) % 3 == 0 elif _mask_id == 4: return (row // 2 + col // 3) % 2 == 0 elif _mask_id == 5: return ((row * col) % 2) + ((row * col) % 3) == 0 elif _mask_id == 6: return (((row * col) % 2) + ((row * col) % 3)) % 2 == 0 elif _mask_id == 7: return (((row + col) % 2) + ((row * col) % 3)) % 2 == 0 else: return (col + row) % 2 == 0 那么下一步我们就是将二维码矩阵代入到八种掩码运算中了： penalty_result = [] _matrix_with_mask = [] for mask_id in range(8): level_and_mask_build(mask_id) _matrix = [[None] * self.length for _i in range(self.length)] for x in range(self.length): for y in range(self.length): if self.data_matrix[x][y] is not None: _matrix[x][y] = self.data_matrix[x][y] ^ mask_template(x, y, mask_id) penalty_result.append(penalty(_matrix)) _matrix_with_mask.append(_matrix) 计算得到了八个结果后，我们还需要从里面筛选出最佳的二维码，计算的规则是一套惩罚计算，惩罚值越低，说明二维码越容易被扫码器识别。惩罚计算的规则有四种，四种惩罚只相加得到最终的惩罚值。N1是计算行或列连续相同色块大于5的区域，N2是计算大面积色块的区域，N3是寻找连续四空色块0000连接1011101色块，N4是计算二维码中的黑白平衡。惩罚力度如下图所示： def penalty(__matrix): def cal_n3(___matrix): _count = 0 check_word = ('00001011101', '10111010000') for row in ___matrix: row_str = ''.join(str(s) for s in row) begin = 0 while begin &lt; len(row_str) and check_word[0] in row_str[begin:]: begin += row_str[begin:].index(check_word[0]) + len(check_word[0]) _count += 1 begin = 0 while begin &lt; len(row_str) and check_word[1] in row_str[begin:]: begin += row_str[begin:].index(check_word[1]) + len(check_word[1]) _count += 1 return _count def get_sum(___matrix): num = 0 for v in ___matrix: if v is list: num += get_sum(v) return num + sum(map(sum, ___matrix)) n1 = 0 n2 = 0 n3 = 0 n4 = 0 # N1寻找连续同色块which &gt;= 5 for reverse in range(0, 2): for j in range(reverse, self.length): count = 1 adj = False for i in range(1 - reverse, self.length): if __matrix[j][i] == __matrix[j][i - 1]: count += 1 else: count = 1 adj = False if count &gt;= 5: if not adj: adj = True n1 += 3 else: n1 += 1 # N2寻找m * n的同色块 count = 0 for j in range(self.length): for i in range(self.length): if __matrix[j][i] == __matrix[j - 1][i] and __matrix[j][i] == \ __matrix[j][i - 1] and __matrix[j][i] == __matrix[j - 1][i - 1]: count += 1 n2 += 3 * count # N3寻找连续四空色块0000连接1011101色块 # 一个方向寻找 + 另一个方向(矩阵转置) transposition_matrix = list(zip(*__matrix)) n3 += 40 * cal_n3(__matrix) + cal_n3(transposition_matrix) # N4计算黑色块占比 dark = get_sum(__matrix) percent = dark // pow(self.length, 2) * 100 pre = percent - percent % 5 nex = percent + 5 - percent % 5 n4 = min(abs(pre - 50) / 5, abs(nex - 50) / 5) * 10 return n1 + n2 + n3 + n4 计算得出所有的二维码的惩罚值后，我们选择那个惩罚值最小的作为最终的二维码矩阵。 _best_mask_id = penalty_result.index(min(penalty_result)) self.data_matrix = _matrix_with_mask[_best_mask_id] return _best_mask_id 生成图片 至此二维码矩阵已经生成并选取完毕了，下面的工作就只是按照矩阵生成图片了，由于我还有其他骚操作，下面的代码并不是简单的直接生成图片，不过也包含在里面了，大家可以拉代码下来慢慢看。 def generate(self, path): if self.qrcode is None: self._matrix_to_img() if self.gif_combine: self.gif_qrcode[0].save(path, save_all=True, append_images=self.gif_qrcode, duration=self.duration, loop=0) else: self.qrcode.save(path) return def _matrix_to_img(self, img_mode='1', matrix=None): border = abs(int(self.border)) size = (self.length + 2 * border, self.length + 2 * border) img = Image.new(img_mode, size, img_mode_2_color_map[img_mode][0]) for x in range(self.length): for y in range(self.length): img.putpixel((x + border, y + border), (img_mode_2_color_map[img_mode][0] - self.data_matrix[x][y]) if matrix is None else matrix[x][y]) self.qrcode = img 结束 二维码的原理以及代码实战到这里就完结了，一年前机缘巧合看到了一篇二维码的介绍文章，一时兴起就跟着写了一个Version 1和H纠错级别的二维码生成代码，但是局限性太大了，而且不支持中文编码，就一直想着写成比较通用的库，但是迟迟没有下手。这次终于拿出了半个月时间重温了一遍并且实现了较为通用的版本，另外还添加了一些定制性的东西进去。当然我的解说以及代码不具备权威性，如果有兴趣验证和深入了解的童鞋，可以参考以下的资料和代码： https://github.com/sylnsfar/qrcode https://coolshell.cn/articles/10590.html https://zhuanlan.zhihu.com/p/21463650 https://www.thonky.com/qr-code-tutorial/ https://www.jianshu.com/p/8208aad537bb https://github.com/tomhaoye/qrcode/blob/master/pdf/SC031-N-1915-18004Text.pdf 完。]]></content>
      <categories>
        <category>有趣</category>
      </categories>
      <tags>
        <tag>other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极验滑动验证码行为模拟]]></title>
    <url>%2F2018%2F09%2F01%2F%E6%9E%81%E9%AA%8C%E6%BB%91%E5%8A%A8%E9%AA%8C%E8%AF%81%E7%A0%81%E8%A1%8C%E4%B8%BA%E6%A8%A1%E6%8B%9F%2F</url>
    <content type="text"><![CDATA[为了能看到具体的效果，使用了selenium+chrome进行实验，如果大家想要打包下载即用，可以自行修改搭配PhantomJS食用。 经过一番折腾成功率已经比较高了（70%-90%吧【2018/10/01】），某些背景干扰元素过多缺口又刚好在色差较小的地方的时候可能会计算错误，也可能还有某些特殊的情况，GIF是旧的，具体可看MP4。 代码仅供参考学习交流，有兴趣可以拿去玩玩，鉴于本人能力有限，代码、逻辑以及解决方式上有错误在所难免，请各位大佬多多指教~ 另外我发现极验官网快改版了，鉴于该代码直接在极验官网上模拟，可能很快就用不了了，届时可以考虑接入api到自己本地再进行模拟测试。 仓库地址: https://github.com/tomhaoye/geetest-crack-demo 故事起源 之前写了个爬虫抓取58的小区数据的代码，有人提了个issue，说小区详情数据和经纬度为空，我自己试了一把，发现请求太频繁会被强制跳转到验证码的页面去。 后来试用了代理，发现一个ip没抓几条数据就被强制跳转人机校验了，然后我手动去访问网页并通过验证码的校验之后再去抓取数据，发现抓取了几千条数据也没有再被强跳了。 这是什么规则=。=算了，还是先想办法能自动通过人机校验先。而58的这个页面的人机校验，是类似于极验的那种滑动验证码。emmmm，网上资料都很多，但是版本较旧，现在的极验滑动校验改进了不少地方，不过思路总是可以参考了。 步骤分解 屏幕出现滑动行为验证码框 截取验证码合成图片\获取验证码原图片 图片处理以及缺口定位 将拼图移动到缺口处 后续：关于干扰块排除的想法 后续：关于增加定位模拟难度的想法 开始研究 0、屏幕出现滑动行为验证码框 我首先找到使用极验控件的地方，例如：极验官网，然后点击到滑动行为验证框出现。这个行为使用selenium模拟比较简单，关于如何使用selenium进行元素定位、模拟点击等这里就不多介绍了，有需要学习的找谷歌百度皆可，有前端基础的一小时内就能够上手。 1、截取验证码合成图片\获取验证码原图片 1.1、获取验证码原图片 现在滑动行为验证框已经出现了，接下来应该是想办法拿到原图片，或者是直接截取当前合成好的图片，两种做法有本质上什么区别呢？首先先说一下获得原图片，极验在加载校验控件的时候会请求三张图片，其中一张是缺口原图，另外一张是完整原图，还有一张是拼块。 缺口原图 完整原图 大家可以看到，请求的所获取到的原图，都是打散重组过的图片，我读过一些解析的文章，有的提到重新组合到一起的方法写在代码里面，而有的文章则是表示散块映射关系是有接口请求的，这也许是大家研究的版本不一样所导致的。我下载了一些原图的组合进行了观察，倒是发现了每一组原图的其实打散前后的散块的位置应该是固定的，也就是说映射关系应该是固定的。其实我觉得，无论映射关系是在前端代码里，还是在接口里，只要你需要在客户端进行重组的，那就相当于没有秘密，所以多一事不如少一事，直接使用固定的映射关系倒是省事不少。 1.2、截取验证码合成图片 这一小节是关于获取图片的，图片处理会在下一小节，所以接下来说一下直接截取合成的图片。所谓所见即所得，能够获取到人眼直接看到的图像，才是最接近人类行为的模拟，而如果通过其他方式取巧，哪天他的方式改了，你的逻辑就不适用了。就像我后来观察58的滑动行为验证，它家请求的图片就是完整的合成图，所以遵循正常人类行为才是究极奥义啊。至于如何截取动行为验证框中的合成图片，我们需要定位到验证框所属的位置，于是乎我先定位到一个class为geetest_window的div，拿到它当前的位置以及长宽属性，接着截图整个当前页面的，并根据刚刚获得的位置以及长宽属性对当前页面图片进行裁剪，就可以获得我们人眼所看到的合成图片了。 def cut_gt_window_image(browser): image_div = browser.find_element_by_class_name(&quot;geetest_window&quot;) location = image_div.location size = image_div.size top, bottom, left, right = location['y'], location['y'] + size['height'], location['x'], location['x'] + size[ 'width'] screen_shot = browser.get_screenshot_as_png() screen_shot = image.open(BytesIO(screen_shot)) captcha = screen_shot.crop((left, top, right, bottom)) captcha.save(cut_image_path) return browser 合成图片 通过这一小节我们对于获取图片的方式和方法已经了然于胸，现在就让我们进入下一小节，对刚刚所获得的图片进行分析和研究吧。 2、图片处理以及缺口定位 2.1、处理打散的原图片与定位缺口位置 在上一小节，我们已经猜测过是打散过的图片跟实际上正常的图片的映射关系是固定的，所以我先尝试对其中一张缺口原图的打散图片进行重组。重组的关键其实就是找到散块跟原来位置的映射关系就行了，于是我拿着打散图片跟合成好的图片一块一块的对比，发现了它实际上是将正常的图片分为上下两部分，然后上下两部分再分为26份进行打散的。关于位置的映射关系，由于时间关系，我就动用了我的火眼金睛直接得到了结果： {1: 18, 2: 17, 3: 15, 4: 16, 5: 22, 6: 21, 7: 14, 8: 13, 9: 10, 10: 9, 11: 19, 12: 20, 13: 2, 14: 1, 15: 6, 16: 5, 17: 26, 18: 25, 19: 23, 20: 24, 21: 7, 22: 8, 23: 3, 24: 4, 25: 11, 26: 12} 这里的映射关系是图片上半部分的，而下半部分的映射关系，我找了前两个散块的原本对应位置，就大概能猜到其实就是使用上半部分的映射关系相邻两值交换后的结果，例如上半部分是1:18, 2:17那么到了下半部分就是1:17, 2:18，下面就是具体的还原图片代码： def merge_img(img_path='', target=''): im = image.open(img_path) to_image = image.new('RGB', (260, 160)) dx = 12 dy = 80 x = 0 img_map = {1: 18, 2: 17, 3: 15, 4: 16, 5: 22, 6: 21, 7: 14, 8: 13, 9: 10, 10: 9, 11: 19, 12: 20, 13: 2, 14: 1, 15: 6, 16: 5, 17: 26, 18: 25, 19: 23, 20: 24, 21: 7, 22: 8, 23: 3, 24: 4, 25: 11, 26: 12} while x &lt;= 300: y = 0 while y &lt;= 80: from_img = im.crop((x, y, x + dx, y + dy)) second_line = img_map[(x / 12) if ((x / 12) % 2) else (x / 12 + 2)] - 1 loc = ((img_map[x / 12 + 1] - 1) * 10 if y else second_line * 10, abs(y - dy)) to_image.paste(from_img, loc) y += dy x += dx to_image = to_image.convert('L') to_image.save(target) return to_image 补充一点，大家看到的打散图片的宽度是312个像素的，而我这里最后还原的得到的图片宽度却是260像素，这是因为打散图片的每一个相邻散块之间实际上是有重叠部分的，一开始我合成得到的宽度312像素的图片时候发现图片是变了样的，所以最后将他们重叠部分堆叠在一起，才得到了正常的原始图片。 还原完整图片 还原缺口图片 缺口图和完整图都可以如法炮制，最后为了方便对比，进行了灰度化处理。接下来由于自身对各种图片格式并不熟悉，碰了不少壁，我会另外再写一篇关于图片格式的学习笔记，完工后会把链接贴过来，有需要的童鞋可以自取，这里我就不详细说自己怎么坑自己的了。 那我们来进入正题，两张完整的图片已经到手了，接下来就应该是定位缺口的起始位置了。童鞋们可以用肉眼看到，两张图片除了缺口位置，其他部分基本是完全一样的，所以思路很简单，就是对两张图片逐个像素点进行对比，然后第一个有差别的位置，是不是就是缺口的起始位置呢？ 换作先前的版本或许是对的，但现在来说大概是错的，为什么这么说呢？比较多前辈的文章中提到第一个差异的位置就是缺口的起点，但是那是基于打散图片后使用的压缩标准是无损压缩，如果使用了有损压缩（或者说支持有损压缩的标准），那么得到的缺口图片和完整图片就不一定只有缺口部分有差异了。不过好在我们知道，即使是有损压缩，也只是损失部分细节，肉眼上看到的差异并不明显，对于计算机来说，就是颜色变化并不大，对于灰度图片来说，我们可以认为是灰度值相差较小。而我们缺口位置，因为需要通过人眼能够清晰的辨别，所以它跟周围的灰度值相差应该是比较大的。于是我们就能够想到去找到这个灰度值相差的阈值，去进行区分到底是缺口还是因为有损压缩所带来的细节损失。分析大概就是到这了，下面是具体的代码： def enlarge_diff_image(bg_path='', fbg_path='', save_path=''): bg_img = image.open(bg_path) fbg_img = image.open(fbg_path) img = image.new('L', (260, 160)) for i in range(260): for j in range(160): if abs(bg_img.getpixel((i, j)) - fbg_img.getpixel((i, j))) &gt; 40: img.putpixel((i, j), bg_img.getpixel((i, j))) else: img.putpixel((i, j), 255) img.save(save_path) 差异瞄点图 上面的图片就是自己坑自己的代表作，因为我将打散图还原后使用了有损压缩，最后对比的结果出来了很多干扰点，虽然通过一定的规则去定位缺口的实际位置没有太大问题，但是大家还是尽量使用无损压缩的图片进行对比会比较妥当。当然你们可以不将还原后的图片保存就直接进行对比，我这里为了方便步骤分解，躺了一次坑，学到新姿势，倒不是坏事。 无损差异瞄点图 得到这样的图片，大家想要求缺口起始位置与左边框的像素就十分简单了，当然我们还可以继续处理，将图片真正的二值化，不过最后求移动距离都是只要求第一个0点（黑点）的像素点就行，以防万一还可以加上对范围内0点（黑点）面积阈值判断。 二值化图 缺口图和完整图的缺口定位并不太难，大概到这里就可以结束了，然我们进入下一小小节。 2.2、处理截取的合成图片与定位缺口位置 这部分的图片的处理和分析对我来说应该是这个小demo里面最难的点了，我知道现在的人工智能领域已经比较强大了，对很多图片内容的认知甚至超过了人类，解决这个问题或许很简单。而我则是没有深入学习过这些方面的知识，但依然想凭借自己现有的技能和思想去解决这个问题。因为前面也说过，这种方式的分析才是最接近人类行为的，只要人类认知图片内容的方式不发生变化，这里面的逻辑就还能用，于是就风风火火的开干吧。 说干就干也不能立刻就写代码，毕竟我们还不知道要怎么对一张图片进行处理和分析，才能够找到那个我们需要的结果。还是先想几条路子出来吧： 缺口和周围有明显的颜色区别，我们能不能以此来确定接口位置？ 滑块和缺口都是比较规矩的拼图形状，而且y轴的范围是一样的，我们能不能在水平方向上找到两条或多条相似的竖线？ 基于上面的想法，我也是尝试做了两套方案出来。 方案一：反正就是想办法把缺口涂黑 好饿好困啊，明天继续写。 好了今天搬完砖回家继续码字。这一方案的最初的想法就是直接根据缺口的颜色范围来确认缺口到底在图中哪个地方，但是后面实现起来却发现是有不少问题的，到最后这个方案所得出来的位置虽有些时候确实能成功，但是准确率比较低。具体问题如下： 大家看到缺口处的颜色大概是由背景加上一层有透明度的灰色渐变图层所组成的，而组合出来的颜色很大程度取决于背景颜色，所以颜色范围就太大了，很容易把背景其他内容也选出来。 有那么两张图背景是偏暗的，如果说缺口没有明显边界的话，那么缺口其实通过肉眼也比较难观察出来，那么这种情况下我们最终描绘出来的图片可能是一大片都是缺口区域，因为缺口和背景融为一体了。 下面是这个方案的代码，没有太多次的去修改，因为颜色范围确实是不可控的，后来干脆也灰度化再对一定灰度范围描绘了。 def get_bin_image(img_path='', save_path='', t_h=150, t_l=60): img = image.open(img_path) img = img.convert('L') table = [] for i in range(256): if i in range(t_l, t_h): table.append(0) else: table.append(1) binary = img.point(table, '1') binary.save(save_path) 二值化结果图 emmmm，得到这个图片，其实是比较理想的情况，缺口周围没有太多的干扰元素，定位缺口的还是比较容易的。大家认真数过可以知道，缺口和拼图的宽度（不算凹凸部分）是42个像素，而拼图与边框的左边距是6像素（拼图突出部分不在左边的情况下），那么缺口的x轴范围就应该是从第49个像素开始。那么我们在这范围内进行缺口起点的查找，可以沿用之前的想法，就是第一个满足一定规律(连续的黑点数量或者比例、范围内黑点面积占比等等)的黑点，我们可以认为他比较大概率是缺口的起点。具体的代码： def get_x_point(bin_img_path=''): tmp_x_cur = 0 img = image.open(bin_img_path).load() # 缺口出现范围大概在x轴[49-52]-220,y轴15-145 for y_cur in range(15, 145): b_acc = 0 tmp_x_cur = 0 for x_cur in range(49, 220): if img[x_cur, y_cur] == 0: if b_acc == 0: tmp_x_cur = x_cur b_acc += 1 else: if b_acc in range(36, 44): return tmp_x_cur - 40 + b_acc else: b_acc = 0 return tmp_x_cur 这个函数我后来没有继续去优化了，因为觉得想到了方案二的一些雏形，就开始着手方案二了。这个函数定义的规则相当的简陋，即使排除了上面提到的问题，有的时候都还是不能准确判断起始位置。大家有兴趣的话可以自己定制一些规则，尝试提升这种方案的成功率。 方案二：对图片所有内容进行描边 关于这个方案，其实一开始并没有觉得比第一个方案好到哪里去，最后试验得出的结果却较为满意，我认为应该还是归功于较为完善的规则。无论是哪个方案，只要是模仿人类认知行为的，都有一定的可行性，尽管方式上有区别，但是这就跟人类认知事物的过程是一样的。例如我们在认识鸡这种动物的时候，既记住了它们的棕黄色的毛，红色的冠，也记住了它们的外形，那我们在下次看到一只乌鸡的时候，我们能够通过它的外形确认这是一只鸡，只是一只颜色不一样的鸡而已，没人会因为它的毛是黑色的而觉得它是一只黑猫。通过捕捉人类区分事物特征点的方式去思考以及编码，才是解决这次实验的关键。 好了，又到了新的一天，今天我继续说说方案二的实验过程。在这节一开始我就简单的描述过方案二，不过可能表述得并不清晰，这里再简洁的说一下，其实第一步，就是先对图像内有明显边界的东西进行描边。例子如下： 截取的原图 描边图 这里我们得到的图片跟原图里面的事物轮廓基本相符的，相信大家都能发现他们的相似之处，而不同之处大概表现在了内容的颜色上面。得到的黑白图片对于我们很有帮助，因为在这里它所表达的信息对于我们来说比五彩斑斓的原图直观得多。下面是代码，具体的方法其实就是通过边界与相邻像素点的颜色差异比较大而得出来的。 def get_contour_image(img_path='', save_path=''): contour_img = image.new('1', (260, 160)) img = image.open(img_path) img = img.convert('L') h_last_point = None v_last_point = None for x in range(260): for y in range(160): if v_last_point is not None and abs(img.getpixel((x, y)) - v_last_point) &gt; 25: contour_img.putpixel((x, y), 1) v_last_point = img.getpixel((x, y)) for y in range(160): for x in range(260): if h_last_point is not None and abs(img.getpixel((x, y)) - h_last_point) &gt; 25: contour_img.putpixel((x, y), 1) h_last_point = img.getpixel((x, y)) contour_img.save(save_path) 既然边界已经描绘出来了，接下来我们就是想办法去定位缺口了。方案二所得到的图片内容比较丰富，基本与原图是没有差别的，那我们就不能够通过方案一中最初规则去定位缺口了，因为在里面满足规则的色块很多很多，所以我们应该为方案二量身定做一套新规则。 现在我们需要做的是观察描边图并找到并找到隐藏在其中的一些信息，大家可以看到：缺口附近的描边十分规整，而拼块附近的描边比较粗大；缺口描边内的黑块面积应该跟拼块描边内的黑块面积差不多；缺口跟拼块在y轴上的范围基本是一样的。接下来我们就利用刚刚观察到的这些信息进行分析，看看能不能得出一定的规律，制定出适当的规则。 从上面的信息中，有的可能自身就能够成为规则，而有的则需要进行组合，也许大家能够找到更多的信息，也能制定很多的规则。而我想到的规则，也许存在不完善或者错误的的地方，但我也想在这里跟大家分享，希望大家指点指点。之前我们寻找缺口基本都是整幅图去遍历所有像素点的，这样很容易被其他的元素所干扰。所以我就想能不能先确定一个方向上（例如y轴）的范围，然后就只需要在（例如x轴上）找到满足的点或线，记录起来最后再做筛选。这里我选取了拼块的左白边作为定位y轴范围的依据，因为他开始于x轴上的第7个像素（原图），而x轴上的7之前的像素基本是没有其他元素干扰的，这让定位y轴范围变得比较方便和准确。一般来说，我们只需要找到x轴上第七个像素（索引为6）在y轴上每连续42个像素（拼块长宽度）中哪个范围白点（描边）最多，那么这个范围就是拼块和缺口左描边的范围。当然，事实上还有特殊的情况，先上个代码再来慢慢解释。 def get_start_point(bin_img_path=''): img = image.open(bin_img_path) # 滑块左边位置7px[6\13]处，获取滑块位置 _pixel = 42 _color_diff_list = {} initial_slider_left_x_index_range = range(6, 14) for initial_slider_left_x_index in initial_slider_left_x_index_range: back_color_n = 0 slider_left = {} for y_cur in range(118): color_n = 0 for add_to_next in range(_pixel): color_n += img.getpixel((initial_slider_left_x_index, y_cur + add_to_next)) slider_left[color_n] = y_cur w_color_n_max = max(slider_left) y_start_cur = slider_left[w_color_n_max] print(f'索引{initial_slider_left_x_index}左白值总和:{w_color_n_max}') for add_to_next in range(_pixel): back_color_n += img.getpixel((initial_slider_left_x_index + 1, y_start_cur + add_to_next)) print(f'索引{initial_slider_left_x_index}右白值总和:{back_color_n}') _color_diff_list[w_color_n_max - back_color_n] = initial_slider_left_x_index best_point = _color_diff_list[max(_color_diff_list)] print(f'最佳起点:{best_point}') return best_point 大家可以看到代码中的注释：# 滑块左边位置7px[6\13]处，获取滑块位置，为什么不是刚刚说的第七个像素，而是第七个像素到第十四个像素这样一个范围？因为拼块它有凹凸的部分，而且凹凸的程度还有不同的情况，我相信这也是极验增加机器模拟的难度的一种手段，规矩的图形太容易被定位了。所以这里需要准确的找到拼块的左描边的位置，还需要结合描边右侧的黑点数量进行判断定位。 我们得到了真正的拼块左边描x轴位置，其实也已经得到了拼块和缺口左边描y轴的上的范围。接下来我们只要使用这个y轴范围在x轴上寻找跟拼块左描边白点数基本相等的x值就能够得出缺口起始位置x值，可能会有缺口结束位置x值，也许还有背景描边满足该条件的x值。 def get_x_point_in_contour(bin_img_path=''): img = image.open(bin_img_path) # 拼块外部阴影范围 _shadow_width = 5 _pixel = 42 # 滑块左边位置7px[6\13]处（考虑凸在左的情况），获取滑块位置 slider_left_x_index = get_start_point(bin_img_path) slider_left = {} for y_cur in range(118): color_n = 0 for add_to_next in range(_pixel): color_n += img.getpixel((slider_left_x_index, y_cur + add_to_next)) slider_left[color_n] = y_cur y_max_col = max(slider_left) print(f'滑块左边白值总和:{y_max_col}') y_start_cur = slider_left[y_max_col] print(f'缺口图像y轴初始位置:{y_start_cur}') # 缺口出现范围大概在x轴[48-52+拼块阴影]-220 gap_left = {} for x_cur in range(slider_left_x_index + _pixel + _shadow_width, 220): color_n = 0 for y_cur in range(y_start_cur, y_start_cur + _pixel): color_n += img.getpixel((x_cur, y_cur)) gap_left[x_cur] = color_n _maybe = [] for x_cur in gap_left: if gap_left[x_cur] in range(int(y_max_col * 0.85), int(y_max_col * 1.3)): _maybe.append(x_cur) print(f'找到缺口可能位置{_maybe}') # 没找到暂时返回滑块长度加滑块起始位置 if len(_maybe) == 0: return 42 + slider_left_x_index, slider_left_x_index elif len(_maybe) == 1: return _maybe[0], slider_left_x_index # 多个结果，则找相邻（缺口内不会有太多干扰元素）结果间差距在38-43之间的第一个数 _max_diff = {} for i in range(len(_maybe) - 1): if _maybe[i + 1] - _maybe[i] in range(38, 43): return _maybe[i], slider_left_x_index else: _max_diff[_maybe[i + 1] - _maybe[i]] = _maybe[i] return _max_diff[max(_max_diff)], slider_left_x_index 这部分代码有点重复，拼块左描边白点数以及y轴范围已经在get_start_point函数中求得了，直接返回使用即可。缺口开始的位置可能是x轴索引48到52（根据拼块的真正起始点而定），一直到259-42=217的范围内（代码中我直接写了220），因为拼块的外部阴影造成了多重描边，所以我们还需要剔除掉右描边的阴影部分。大家能看到我定的规则是白点数满足(0.85至1.3)倍的拼块左描边白点数，即视为可能的位置。实际上它们白点数应该是相差极少，甚至可以说是相等的，大家可以通过实验去调优这里的倍数参数，也许还能剔除掉更多的干扰。最后考虑到得到的结果有可能除了真正的起点和终点外，还有其他的一些干扰，所以做了简单的判断，实际上就当前所知的背景图来看，不会出现这种情况。到这里我们已经得到了解决问题的所有钥匙了，接下来就是用钥匙打开新世界大门的时候了。 3、将拼图移动到缺口处 这一节相对比较简单，拖动操作使用selenium的ActionChains来实现，上面得到的缺口起始x值和拼块真实的起点值相减，就得到了移动的实际距离，然后直接拖动过去就可以了？确实可以，可以重头来过了。极验会根据整个移动过程的速度判断你是机器还是人，不是说快慢的问题，而是正常的人类行为在移动中速度是会变化的，最真实的情况就是你在接近缺口的时候会减速。根据物理学原理（早就忘了），我写了一个匀加速运动（实际上并不匀）来模拟人类拖动，因为move_by_offset会自动将偏移量转成整形类型，所以我还是自己先转好了。 def btn_slide(browser, x_offset=0, _x_start=6): # 开始位置右偏6-13像素 x_offset = abs(x_offset - _x_start + 1) slider = browser.find_element_by_class_name(&quot;geetest_slider_button&quot;) ActionChains(browser).click_and_hold(slider).perform() section = x_offset left_time = 1 x_move_list = get_x_move_speed(x_offset, left_time, section) print(f'魔鬼的步伐：{x_move_list}') print(f'实际应该移动距离:{sum(x_move_list)}') for x_move in x_move_list: ActionChains(browser).move_by_offset(x_move, yoffset=0).perform() ActionChains(browser).release().perform() time.sleep(2) browser.close() def get_x_move_speed(distance=0, left_time=0, section=10): origin_speed = distance * 2 acc_speed = origin_speed / left_time / left_time / section move_offset = [] new_speed = origin_speed for i in range(0, section): new_speed = new_speed - acc_speed move_offset.append(round(new_speed / section)) if sum(move_offset) &gt;= distance or (round(new_speed / section)) == 0: break if sum(move_offset) &lt; distance: move_offset.append(distance - sum(move_offset)) return move_offset 最后的实际效果 Your user agent does not support the HTML5 Video element. 好了，我的整个实验就到这里就结束了，下面还有一些其他方面的思考。如果有能看到这里的童鞋，既然来了，不如在评论留下一个脚印？ 4、后续：关于干扰块排除的想法 相信大家都能发现，背景图里面，很多时候会在随机的位置出现另外一个或多个&quot;缺口&quot;，它们的颜色相对真正的缺口来说浅一些，但是对于上面方案二的描边法来说，它们确实也是会被描绘出来，虽然我们利用拼块确定了y轴的范围，但是当这些假的&quot;缺口&quot;也在这个y轴范围内的时候，特别是凹凸和方向完全跟真实缺口一样的时候，我们上面的代码就没办法分辨到底哪个才是真正的缺口了。但是解决这个问题的办法也比较简单，我这里想到的思路有两种： 得到的两个位置获取它们原图颜色，相对较深的为真 所有得到的缺口位置都记录下来，先移动到第一个缺口位置，若是不成功，再移动到下一个缺口位置 不管怎样的干扰，总不会喧宾夺主。因为毕竟是给人类辨认的，而机器，只要你制定适当的规则，它们的辨识能力能够达到远超人类的水准。 5、后续：关于增加定位模拟难度的想法 相信大家看完了实验的整个过程，在这里一定会有很多关于如何设计更加复杂、更有效防止机器模拟行为的验证码的想法，我这里也就这次实验中的滑动验证码发表一下自己的看法。 可以说滑动验证码是国内一种比较新颖的人机校验方式，因为他的操作简单直观，适用于任何年龄层的互联网用户。那实际人机校验的效果如何呢？如果仔细读完上文或者对滑动验证码有研究的童鞋，应该都知道，它实际上是进行了两次校验：识别和行为，我个人认为它能够更好的保护我们的应用以及用户数据，所以现在在国内它的普及度也是相当高。而我这次的实验针对当前版本进行制定规则以实现了机器模拟的目的，但我认为滑动验证码可以通过以下的调整来比较有效增加模拟的难度： 拼块滑动不限于x轴，可在图内任意移动 拼块和缺口的形状改为非固定的多边形或不规矩形状 最最最最后，我想说的是，验证码技术经过多年的发展现在已经相当成熟了，除了日常的数字字母验证码，这次实验的滑动验证码，相信大家也听说过无比牛b的12306验证码，还有google的recaptcha等等。虽然当今的验证码领域已经有如此多的优秀方案，但是我相信没有一个方案能够百分百的抵御机器人，你有世界上最优秀的工程师、科学家，他有这个世界上最有钱的老板啊，把你的工程师和科学家撬过来破解还不是两天的事。现代互联网技术就是在这样的攻防游戏中逐渐的发展壮大的，时代的巨轮滚滚向前，跟不上时代步伐的技术只会被碾压得粉碎。 完。]]></content>
      <categories>
        <category>有趣</category>
      </categories>
      <tags>
        <tag>other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo踩坑记]]></title>
    <url>%2F2018%2F08%2F10%2Fhexo%E8%B8%A9%E5%9D%91%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[hexo的搭建和使用十分的方便，搭配github page食用更佳。github上有很多绚丽的hexo主题，大家可以按需搭配。虽说hexo+github page搭建个人博客很方便快捷，但如果你需要较多的个性化定制，坑总是会踩的。本文主要遇到的各种坑以及记录简单的操作步骤，如果需要详细的安装部署教程请点击传送门离开。 安装hexo 如果你不知道或者未安装 npm ，点击此处了解以及下载。 npm i -g hexo-cli 说到npm我在后面挖到了一个坑，这个坑以前已经挖过，这里又遇到了，有必要记下来。如果你跟我一样是在使用Windows 10 + VirtualBox (VBox) + Vagrant + Laravel Homestead + 共享目录这样的环境做开发的，需要注意。 在上述环境中使用npm会有比较多意料之外的状况，大部分stackoverflow上都有解决方案，但这次遇到的error “ETXTBSY: text file is busy” on npm install弄了一段时间仍然没解决，把npm搞坏后还是换到windows环境去了。 虽然没解决，但还是推荐一下比较靠谱的解决方案。 快速搭建 hexo init [folder] //初始化，新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站 cd folder //进入网站根目录，如果没有设置 folder ，则不需要该步骤 hexo g //generetor的缩写，生成静态文件。 hexo s //server的缩写，启动服务器。默认情况下，访问网址为： http://localhost:4000/。 主题 hexo有很多主题可以提供选择，而我这里则使用了material这个主题，最坑的是他的文档网站ssl证书居然过期了，想看都看不了=。=那就没办法啦，咱一起逐个配置试一下，再问一问Google，应该也不会有啥大问题。 首先我们把material的项目clone下来放到hexo目录下的themes中，根据说明为了避免配置文件被覆盖，我们需要复制一份_config.template.yml并命名为_config.yml。接下来我们的就开始定制我们自己的主题风格。_config.yml中大多数的配置项都有简单的说明，配置完各种基本信息后，我们想要配置一些个性化的东西，例如代码的高亮样式、选择自己喜欢熟悉的评论系统等等。 高亮 首先说说配置代码高亮样式，在备注Code highlight下有两个配置项 # Code highlight # You can only enable one of them to avoid issues. # Also you need to disable highlight option in hexo's _config.yml. # # Prettify # theme: # Available value in /source/css/prettify/[theme].min.css prettify: enable: false theme: &quot;github-v2&quot; # Hanabi (https://github.com/egoist/hanabi) # line_number: [true/false] # Show line number for code block # includeDefaultColors: [true/false] # Use default hanabi colors # customColors: This value accept a string or am array to setting for hanabi colors. # - If `includeDefaultColors` is true, this will append colors to the color pool # - If `includeDefaultColors` is false, this will instead default color pool hanabi: enable: true line_number: true includeDefaultColors: true customColors: 上面是各种高亮的样式选择，而下面则是hanabi，一个很骚的高亮风格（没错就是我用着的这个）。这里需要注意的是这两个配置项你只能设置其中一个enable为true，而且hexo的_config.yml中有一个配置项跟它是冲突的 # Writing new_post_name: :title.md # File name of new posts default_layout: post titlecase: false # Transform title into titlecase external_link: true # Open external links in new tab filename_case: 0 render_drafts: false post_asset_folder: false relative_link: false future: true highlight: enable: false line_number: false auto_detect: false tab_replace: 这里的highlight设置为true会让hanabi不起作用。如果你选用prettify的话，可以在路径material/source/css/prettify/[theme].min.css找喜欢的高亮样式，其中[theme]就是prettify.theme中填写的样式名称。 评论系统 大家可以看到，集成服务里面，包含了评论系统的配置项，如此方便的配置就能够加入一个评论系统到博客里了，何乐而不为呢？ # Comment Systems # Available value of &quot;use&quot;: # disqus | disqus_click | changyan | livere | gitment | gitalk | valine | wildfire # If you want to use gitment or gitalk,you should get the client_id and client_secret from https://github.com/settings/applications/new # If you want to use valine,you should get the app_id and app_key from https://leancloud.cn ,more setting please see https://valine.js.org comment: use: gitalk shortname: # duoshuo or disqus shortname changyan_appid: changyan_conf: changyan_thread_key_type: path livere_data_uid: gitment_repo: # git repo of the hexo gitment_owner: # git repo's owner gitment_client_id: # github app client id gitment_client_secret : # github app client secret valine_leancloud_appId: # leancloud application app id valine_leancloud_appKey: # leancloud application app key valine_notify: false # valine mail notify (true/false) https://github.com/xCss/Valine/wiki valine_verify: false # valine verify code (true/false) valine_pageSize: 10 # comment list page size valine_avatar: identicon # gravatar style https://valine.js.org/#/avatar valine_lang: zh-CN # i18n valine_placeholder: Just go go # valine comment input placeholder(like: Please leave your footprints ) valine_guest_info: nick,mail,link #valine comment header info gitalk_repo: gitTalk gitalk_owner: tomhaoye gitalk_client_id: ksjhdfkhfksdhfjsdhf gitalk_client_secret: jd12809j890sjhdosihdoaishd9o21d9081h2yd0 wildfire_database_provider: firebase # firebase or wilddog wildfire_wilddog_site_id: wildfire_firebase_api_key: wildfire_firebase_auth_domain: wildfire_firebase_database_url: wildfire_firebase_project_id: wildfire_firebase_storage_bucket: wildfire_firebase_messaging_sender_id: wildfire_theme: light # light or dark wildfire_locale: zh-CN # en or zh-CN 大家可以看到，集成的评论系统有：disqus，disqus_click，changyan，livere，gitment，gitalk，valine，wildfire，大家喜欢哪个就就配置对应的配置项就可以了。例如我使用的gitalk，只需要在个人设置中选择开发者设置，然后选择Oauth Apps，新建一个授权应用，将得到的Client ID和Client Secret填到上面的gitalk_前缀配置项中，gitalk_repo填写你的应用名称，gitalk_owner填写你的账号名即可。 当然配置不是重点，这篇文章主要是记录踩过的坑，没趴下过的地方我也不会特别提出来。我完完整整的配置好了我的的gitalk评论系统后，先新建了一片文章并打开链接，评论系统自动请求并在github的仓库里面新增了一个issue，据我猜测这个issue就是用来记录当前文章的所有评论的。果不其然，我在发表了一条评论，issue里面便新增了留言一段记录。嗯，一切都十分的顺利，于是我又新建了一篇名字比较长的文章并打开链接，咦？怎么请求发生了错误？后来在Google查了半天，终于发现gitalk的初始化id参数最长只能传50个字符，而它默认是直接拿当前的url作为参数的，所以请求会返回客户端错误。知道了问题所在解决就比较简单了，只需要在theme/material/layout/_widget/comment/gitalk/main.ejs中加入： &lt;script&gt; var gitalk = new Gitalk({ clientID: '&lt;%= theme.comment.gitalk_client_id %&gt;', clientSecret: '&lt;%= theme.comment.gitalk_client_secret %&gt;', repo: '&lt;%= theme.comment.gitalk_repo %&gt;', owner: '&lt;%= theme.comment.gitalk_owner %&gt;', admin: ['&lt;%= theme.comment.gitalk_owner %&gt;'], id: document.title.substr(0,50), // facebook-like distraction free mode distractionFreeMode: false }) gitalk.render('gitalk-container') &lt;/script&gt; 这里我是将title截取最多50个字符，大家也可以结合自身情况传入自己需要的参数。 Emoji表情渲染 Hexo默认的markdown渲染引擎不支持emoji表情的渲染，为了美化博客，我们可以更换一个支持emoji的渲染引擎，并添加一个emoji插件。方法很简单，只需要键入以下三行命令： npm un hexo-renderer-marked --save npm i hexo-renderer-markdown-it --save npm install markdown-it-emoji --save 全部安装成功后在Hexo的_config.yml配置文件中加入以下的配置： ## markdown 渲染引擎配置，默认是hexo-renderer-marked，这个插件渲染速度更快，且有新特性 markdown: render: html: true xhtmlOut: false breaks: true linkify: true typographer: true quotes: '“”‘’' plugins: - markdown-it-footnote - markdown-it-sup - markdown-it-sub - markdown-it-abbr - markdown-it-emoji anchors: level: 2 collisionSuffix: 'v' permalink: true permalinkClass: header-anchor permalinkSymbol: '' 在emojy-cheet-sheet中找到你想要的表情编码，例如笑脸对应的emoji编码是:smile:，清除旧文件并重新构建后在你的文章中就可以会出现😄了。 没有结束 由于博客还在搭建美化中，该文章会持续更新。。。]]></content>
      <categories>
        <category>挖坑</category>
      </categories>
      <tags>
        <tag>other</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP Standard Recommendation]]></title>
    <url>%2F2018%2F06%2F16%2FPHP-Standard-Recommendation%2F</url>
    <content type="text"><![CDATA[非官方规范。 PSR-0(AutoLoading Standard) (14年10月21日起标记为deprecated，由PSR-4代替) 完全合格的命名空间和类名必须有以下结构 &quot;\&lt;vendor name&gt;\(&lt;Namespace&gt;)*&lt;Class Name&gt;&quot; 每个命名空间必须有顶级命名空间 (&quot;Vendor Name&quot;) 每个命名空间可以有任意多个子命名空间 每个命名空间在被从文件系统加载时必须被转换为操作系统的路径分隔符 每个&quot;_“字符在类名中被转换为DIRECTORY_SEPARATOR。”_&quot;在命名空间中没有明确含义 符合命名标志的命名空间和类名必须以&quot;.php&quot;结尾来加载 VendorName、命名空间、类名可以由大小写字母组成，其中命名空间和类名是大小写敏感的以保证多系统兼容 PSR-1(Basic Coding Standard) 源文件必须只使用以下这两种标签 &lt;?php 和 &lt;?= 源文件中php代码的编码格式必须只适用不带BOM的UTF－8（BOM——字节顺序标记） 一个源文件建议只用来做声明（类、函数、常量等）或者只用来做一些引起副作用的操作（如输出信息，修改配置文件等），但不应该同时做这两件事 命名空间和类必须遵守PSR-0 类名必须使用StudlyCaps写法 类中的常量必须只由大写字母和下划线组成 方法名必须使用camelCase写法 PSR-2(Coding Style Guide) 代码必须遵循PSR-1 代码必须使用4个空格进行缩进，而不是制表符 一行代码的长度不应该有硬限制，软限制为120个字符，建议每行小于80 在命名空间声明下必须空一行，use下同理 类的左、右花括号必须各自成一行 方法的左右花括号都必须各自成一行 所有属性、方法必须有可见性声明；abstract和final必须在可见性声明前，static必须在可见性声明后 在结构控制的关键字后必须空一格；函数调用后面不可有空格 结构控制关键字左花括号必须同一行，右花括号必须放在代码主体下一行 控制结构的左花括号之后不可有空格，右花括号之前也不可有空格 PSR-3(Logger Interface) LoggerInterface暴露八个接口用来记录八个等级(debug,info,notice,warning,error,critical,alert,emergency)的日志 第九个方法是log，接受日志等级操作为第一个参数。用一个日志等级常量来调用这个方法必须和直接调用指定等级方法的结果一致。用一格本规范中来定义且不为具体实现所知的日志等级来调用该方法必须跑出一个 PSR\Log\InvalidArgumentException 不推荐使用自定义的日志等级，除非你非常确认当前类库对其支持。 PSR-4(Improved AutoLoading) (兼容PSR-0) 术语［类］是一个泛称，它包含类、接口、trait及其他类似结构 完全限定类名应该如下范例 &lt;NamespaceName&gt;(&lt;SubNamespace&gt;)*&lt;ClassName&gt; 完全合规类名必须有一格顶级命名空间 完全合规类名可有多个子命名空间 完全合规类名应该有一格终止类名 下划线在完全合规类名中是没有特殊含义的 字母在完全合规类名中可以是任何大小写组合 所有类名必须以大小写敏感的方式引用 当完全合规类名载入文件时： 在完全合规类名中，连续的一个或几个子命名空间构成的命名空间前缀（不包括顶级命名空间分隔符），至少对应一格基础目录 在［命名空间前缀］后的连续子命名空间名称对应一个［基础目录］下的子目录，其中的命名空间分隔符标示目录分隔符。子目录名称必须和子命名空间名大小写匹配 终止类名对应一个以.php结尾的文件。文件名必须和终止类名大小写匹配 自动载入器的实现不可以抛出任何异常，不可以引发任何等级的错误，也不应该有返回值]]></content>
      <categories>
        <category>规范</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO模型笔记]]></title>
    <url>%2F2018%2F05%2F20%2FIO%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[IO Model 常见IO模型 blocking IO nonblocking IO IO mutiplexing(select &amp; poll) signal driven IO asynchronous IO(the POSIX aio_functions) 因为七牛bucket删除了，图示没了，重新作图日后补上 一个基本的IO，它会涉及到两个系统对象，一个就是系统内核，另一个是调用这个IO的对象。当一个read操作发生时，会经历以下阶段： 通过read系统调用向内核发起读请求 内核向硬件发送读指令，并等待读就绪 内核把将要读取的数据复制到描述符所指向的内核缓存区 将数据从内核缓存区拷贝到用户进程空间中 同步与异步 同步和异步关注的是消息通信机制 注：同步IO过程由进程处理，异步IO交由内核处理IO 阻塞与非阻塞 阻塞和非阻塞关注的是程序在等待调用结果时的状态 1，2，3，4属于同步IO，5属于异步IO AIO异步非阻塞IO，适用于连接数多且IO时间长的架构，如相册服务器，JDK7开始支持 NIO同步非阻塞IO，适用于连接数多且轻操作（IO?）的架构 BIO同步则色IO，适用于连接数少且固定的架构 一般来说，IO主要有两种情况（服务器）：一是来自网络的IO，二是文件的IO。windows提供异步IO，Linux提供epoll模型给网络IO，文件IO则提供AIO epoll,select/poll 本质上都是同步IO，自己处理IO过程，但是epoll优化了轮询操作，使用callback机制响应。 额外提供Edge Triggered，用户空间可能缓存IO状态，减少epoll-wait／epoll-pwait调用 level triggered &amp; edge triggered LT事件不会丢弃，只要读buffer里面有数据可以让用户读，就会不停的通知。而ET则只发在事件发生之时通知。 select缺点： 每次调用，都需要把fd集合从用户态拷贝到内核态，fd多事件开销大 每次都要遍历进入内核的fd，开销也很大 select支持fd数量太小，默认1024 poll与select只在fd集合的结构上面有区别 epoll对select缺点的改进 新事件注册到epoll句柄中，会把所有的fd拷贝进内核，而不是在epoll_wait时重复拷贝 为fd指定回调函数，设备就绪，调用回调函数唤醒等待者，并将fd加入就绪链表 最大的限制很大程度上跟系统内存大小有关 消息传递：mmap加速 总结 select、poll都需要轮询遍历所有fd，而epoll只需要读取就绪链表 select、poll每次调用都copy一次fd，并往设备队列上挂。而epoll只要copy一次fd，并只往自己的等待队列上挂一次即可]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux添加服务到开机自动启动]]></title>
    <url>%2F2018%2F04%2F22%2FLinux%E6%B7%BB%E5%8A%A0%E6%9C%8D%E5%8A%A1%E5%88%B0%E5%BC%80%E6%9C%BA%E8%87%AA%E5%8A%A8%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[Systemd 是 Linux 系统中最新的初始化系统，Systemd 服务文件以 .service 结尾。一些使用包管理工具安装的软件会自动建立 .service 服务文件，路径在 /lib/systemd/system/ 下，但自行建立及管理的文件建议放在 /etc/systemd/system/ 目录下。内容以 supervisor 为例： [Unit] Description=Supervisor process control system for UNIX Documentation=http://supervisord.org After=network.target [Service] ExecStart=/usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf ExecStop=/usr/bin/supervisorctl $OPTIONS shutdown ExecReload=/usr/bin/supervisorctl -c /etc/supervisor/supervisord.conf $OPTIONS reload KillMode=process Restart=on-failure RestartSec=50s [Install] WantedBy=multi-user.target 参数说明： [Unit] Description：描述服务 Documentation：参考资料 After：描述服务类别 [Service] Type：是后台运行的形式 ExecStart：服务的具体运行命令 ExecReload：重启命令 ExecStop：停止命令 KillMode：daemon终止时所关闭的程序 Restart：触发重启 RestartSec：重启等待时间 TimeoutSec：无法顺利启动强制关闭时间 注意：[Service]的启动、重启、停止命令全部要求使用绝对路径 [Install] 运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3 如过想要某些服务开机启动，例如以 php-fpm 为例，编写service： sudo vi /etc/systemd/system/php7.1-fpm.service [Unit] Description=The PHP 7.1 FastCGI Process Manager Documentation=man:php-fpm7.1(8) After=network.target [Service] Type=notify PIDFile=/run/php/php7.1-fpm.pid ExecStart=/usr/sbin/php-fpm7.1 -R --nodaemonize --fpm-config /etc/php/7.1/fpm/php-fpm.conf ExecReload=/bin/kill -USR2 $MAINPID [Install] WantedBy=multi-user.target 随后如果 php-fpm 在运行则先将其关闭，运行： systemctl daemon-reload systemctl start php7.1-fpm.service 测试能够成功开启服务了，就可以将服务设置为开机启动： systemctl enable php7.1-fpm.service 常用命令 systemctl daemon-reload systemctl list-units --type=service systemctl list-unit-files --type=service systemctl start unit.service systemctl stop unit.service systemctl restart unit.service systemctl enable unit.service systemctl disable unit.service systemctl is-enable unit.service systemctl is-active unit.service systemctl status unit.service]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年游记——贵州]]></title>
    <url>%2F2017%2F09%2F15%2F2017%E5%B9%B4%E6%B8%B8%E8%AE%B0%E2%80%94%E2%80%94%E8%B4%B5%E5%B7%9E%2F</url>
    <content type="text"><![CDATA[读万卷书，走万里路 前言 大学时期一开始过得有点迷茫，上上课、去去社团，等到能码两行代码的时候，就开始跟老师同学搞搞小项目，眨眼间两年过去了，感觉生活有点平淡。萌生了想出去走走的想法，然而当计划到一半的时候，机缘巧合接触到了一个感兴趣的创业项目，于是跟随着一位师兄创业去了，一直到毕业一年后才想起当初对旅行的憧憬，于是决定重新计划一下路线和行程。 前前后后一共花了一个多个月的闲余时间去准备，确定了自己的大概路线，然后查找每个地方的攻略，接着查地图、交通等等。终于在在九月中旬，我一大早到达了广州南站，搭上了开往贵阳的列车。 第一站，贵阳 我这次的路线规划决定以中国西北部省市为主，加以拷问了自己的内心，得出了几个最想去的地方，而贵阳是离我最近的一个。在车上看看风景听听歌睡睡觉，4个小时的时间很快就过去了，而我也顺利到达了贵阳高铁站。之前预定的青旅离高铁站很近，走了大概15分钟就到了。 提前打电话给了青旅老板，很快在小区里面就碰面了。老板领着我去登记了信息后就带我到我的房间去了，聊着聊着才发现这个地方属于郊外，这整个小区基本都是被各种老板买下来做短租旅馆之类的，旺季的时候基本每个房间都塞满人，但现在属于淡季，所以晚上附近很少人，尽量少在晚上外出，平时外出也尽量在天黑前回来。因为到达的时间是中午，我本来是打算先在下面的餐馆吃一顿就回青旅睡一觉，晚上再出去走走的，不过既然老板这样说了，那我就决定吃完后就往城里出发吧。 虽然是郊区，但是交通还是很便捷，几乎所有公交车都可以到达市中心，车票也比较便宜，1块钱就行了。之前大概看了到达市中心的路程，大概需要一个小时，虽然路程是比较远，但是因为内心充满了好奇，所以一路上都觉得很精神。贵阳很多小山，所以地势起起伏伏，很多时候建筑都是依山而建的。不过这里大多数山都很矮，不像重庆那样，动不动几十层高。差不多四点钟的时候到了市中心的商业区，尽管现在是淡季，但是商业区很繁华，在附件商场逛了一下，一路上买点小吃吃，不过我在贵阳对吃的记忆并没多少，可能这里的风味都较为普通，也可能是我没有吃到真正的特色菜吧。 逛着吃着很快就到了晚上了，走到外面街上，周围的灯光色彩斑斓。咦，好像忘了点什么？噢，对了！要早点回去！不过既然都夜晚了，而且夜景这么好看，再看一阵子吧。😂 夜景虽好，但不能贪杯。否则，回过神来，就会发现已经九点多了。赶紧赶上车回程，快十一点才终于回到了青旅。之前老板跟我说今天这里只有你和另外一个人（长住），而且我们是两个房间的，我看时间也比较晚了，所以也没去打扰他，洗漱完之后遍躺上床睡觉了。 第二站，凯里 😄新的一天新的开始，第二天的行程是去西江千户苗寨，高铁票也是之前就订好了，同样是一大早到了高铁站（其实一点都不早😂）。 车跑得很慢，慢得不像高铁，但是路程很短，半小时的时间就到了。 一出高铁站，迎面而来的是各种拉客拼车的师傅，我之前看过攻略说过这种应该都是非法营运的，其实附近就有正规的旅游专线，于是我跟随大多数旅客的步伐，去附近的服务站报了个名，等待十人成团后便发车了。 大概一个多小时的车程，我们大概在十二点的时候到达了目的地。虽然不是很饿，但是景区大门有很多的小食摊，于是我决定先吃为敬。 吃饱喝足后，准备进寨子里去了。想进寨，先买票，买票的方式很多种，包括但不限于微信、支付宝、美团。票的种类也很多，包括但不限于单人门票、团体门票、单人套票、团体套票。令我感到意外的一点是，广东的身份证还能享受优惠！ 买完票排个队很快就进了寨子入口，然后会有个巴士会统一载到寨子里面去。 到达寨子后就大家就解散了，而我就先前往预定好的青旅里面先做好登记。 虽然看上去景点人超级多，但是选择青旅的游客貌似很少，前台帅哥告诉我今天只有三个人入住，而我的房间里面只有他和我。😂青旅里面有几只猫，但是可能平时被撸烦了，摸一下就跑掉了，很高冷。 猫好像是撸不了了，那就出去逛逛吧。把贵重的物品和日常用品放进小书包后，就往外走了，前台帅哥很贴心的跟我说如果晚上找不到路回来的话，就微信一下他，他会发个定位过来。我让他不用担心，因为我本来就是地图定位过来的哈哈，寒暄完我就大刀阔斧往外走了。 跟大多数古城一样，商业化的气息其实还是满足的，不过这样我觉得挺好，至少寨子里面就能够满足游客和居民的日常需求，不会要什么没什么。寨子其实挺大，再加上淡季的原因，所以在街上看到的人流密度比较低。不过在热门的观赏点，还是挤满了男女老少。 上图就是一个比较热门的观赏点，从寨子里面走上去大概要二十来分钟，寨子也提供观赏车服务，前两次（来回）免费（包含在票价里），后面好像是每次15块钱。逛着逛着很快就已经到晚上了，门票还包含了今晚的晚饭哈哈，晚饭吃了什么我是不记得了，不过戏倒是挺足的，重要的是我们也能参与其中。 吃饱看足后我变再次乘车前往全景观赏点了，因为晚上的灯饰会比较好看，所以排队的人数也很多，于是我决定跟着大多数人的步伐一起走上去。到了观赏点后发现人不是一般的多，十分艰难的挤到了围栏边，随手拍了一张： 😂其实是加了滤镜，不过观赏点人太多挤来挤去的拍照是真不容易。下面给大家看一下原滋原味的吧： 喂饱了手机就回青旅了，归程排队等车的人依然很多，所以我还是走回去了，也好慢慢的欣赏一下各个角度的苗寨。晚上十二点多外面依然很热闹，灯火也还很旺，一直到了一点多的时候才安静下来。 第二天起床，打包了还没干透的衣服，收拾好所有的行李，准备回贵阳然后转乘去重庆的火车。今天阳光正好，一扫昨天的阴霾，出门的时候大橘和小黑正在慵懒的晒太阳。 我选择了拼车去火车站，车上刚好有一对中年夫妻也是去贵阳的，于是路上我们就聊了起来。大哥大姐是北方人，在了广东开了工厂，最近由于一些原因工厂要闭厂一段时间，于是就出来玩了。他们问及我为何贵州就走这么点地方，我是跟他们说：我想去的就必须去，没规划到的，就看心情吧哈哈。然后我再问他们后面准备去哪，他们就回答我说：主要看心情吧，没有必须去的地方，只要两个人在一起就好了。这不是摆明虐我吗😂]]></content>
      <categories>
        <category>闲聊</category>
      </categories>
      <tags>
        <tag>other</tag>
      </tags>
  </entry>
</search>
